{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "2_word2vec.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyungminkim-dev/boostcamp-ai-tech/blob/main/2_word2vec_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3FAK0fz1kOr"
      },
      "source": [
        "##**2. Word2Vec**\r\n",
        "1. 주어진 단어들을 word2vec 모델에 들어갈 수 있는 형태로 만듭니다.\r\n",
        "2. CBOW, Skip-gram 모델을 각각 구현합니다.\r\n",
        "3. 모델을 실제로 학습해보고 결과를 확인합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9FrxTPWIsct"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjroCdtwI9Rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a84702e2-ff02-48b7-dbce-77a58a968af2"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.2MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/af/93f92b38ec1ff3091cd38982ed19cea2800fefb609b5801c41fc43c0781e/JPype1-1.2.1-cp36-cp36m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 52.9MB/s \n",
            "\u001b[?25hCollecting tweepy>=3.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/67/c3/6bed87f3b1e5ed2f34bd58bf7978e308c86e255193916be76e5a5ce5dfca/tweepy-3.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.6/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 16.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.6/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: colorama, JPype1, tweepy, beautifulsoup4, konlpy\n",
            "  Found existing installation: tweepy 3.6.0\n",
            "    Uninstalling tweepy-3.6.0:\n",
            "      Successfully uninstalled tweepy-3.6.0\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2 tweepy-3.10.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSP7aXfJIr3i"
      },
      "source": [
        "from tqdm import tqdm\r\n",
        "from konlpy.tag import Okt\r\n",
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "import torch\r\n",
        "import copy\r\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qugro74yJASr"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q36dfSRRJDtX"
      },
      "source": [
        "\r\n",
        "\r\n",
        "데이터를 확인하고 Word2Vec 형식에 맞게 전처리합니다.  \r\n",
        "학습 데이터는 1번 실습과 동일하고, 테스트를 위한 단어를 아래와 같이 가정해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLZ2f-lRJSus"
      },
      "source": [
        "train_data = [\r\n",
        "  \"정말 맛있습니다. 추천합니다.\",\r\n",
        "  \"기대했던 것보단 별로였네요.\",\r\n",
        "  \"다 좋은데 가격이 너무 비싸서 다시 가고 싶다는 생각이 안 드네요.\",\r\n",
        "  \"완전 최고입니다! 재방문 의사 있습니다.\",\r\n",
        "  \"음식도 서비스도 다 만족스러웠습니다.\",\r\n",
        "  \"위생 상태가 좀 별로였습니다. 좀 더 개선되기를 바랍니다.\",\r\n",
        "  \"맛도 좋았고 직원분들 서비스도 너무 친절했습니다.\",\r\n",
        "  \"기념일에 방문했는데 음식도 분위기도 서비스도 다 좋았습니다.\",\r\n",
        "  \"전반적으로 음식이 너무 짰습니다. 저는 별로였네요.\",\r\n",
        "  \"위생에 조금 더 신경 썼으면 좋겠습니다. 조금 불쾌했습니다.\"       \r\n",
        "]\r\n",
        "\r\n",
        "test_words = [\"음식\", \"맛\", \"서비스\", \"위생\", \"가격\"]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vReElaFSLBYL"
      },
      "source": [
        "Tokenization과 vocab을 만드는 과정은 이전 실습과 유사합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTjlRzmWMDK_"
      },
      "source": [
        "tokenizer = Okt()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DTUsX672icp"
      },
      "source": [
        "def make_tokenized(data):\r\n",
        "  tokenized = []\r\n",
        "  for sent in tqdm(data):\r\n",
        "    tokens = tokenizer.morphs(sent, stem=True)\r\n",
        "    tokenized.append(tokens)\r\n",
        "\r\n",
        "  return tokenized"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-z0z6HD2rrX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "493531e2-c865-4b21-9c14-c95a0702b51e"
      },
      "source": [
        "train_tokenized = make_tokenized(train_data)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:05<00:00,  1.99it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51exEpI0Mc3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a394e472-4b91-4de2-9605-7a0b08ccfc23"
      },
      "source": [
        "word_count = defaultdict(int)\r\n",
        "\r\n",
        "for tokens in tqdm(train_tokenized):\r\n",
        "  for token in tokens:\r\n",
        "    word_count[token] += 1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 15557.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyvHAMAnMh1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "effdeb65-acc1-45f0-8978-c49df3673973"
      },
      "source": [
        "word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\r\n",
        "print(list(word_count))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.', 14), ('도', 7), ('이다', 4), ('좋다', 4), ('별로', 3), ('다', 3), ('이', 3), ('너무', 3), ('음식', 3), ('서비스', 3), ('하다', 2), ('방문', 2), ('위생', 2), ('좀', 2), ('더', 2), ('에', 2), ('조금', 2), ('정말', 1), ('맛있다', 1), ('추천', 1), ('기대하다', 1), ('것', 1), ('보단', 1), ('가격', 1), ('비싸다', 1), ('다시', 1), ('가다', 1), ('싶다', 1), ('생각', 1), ('안', 1), ('드네', 1), ('요', 1), ('완전', 1), ('최고', 1), ('!', 1), ('재', 1), ('의사', 1), ('있다', 1), ('만족스럽다', 1), ('상태', 1), ('가', 1), ('개선', 1), ('되다', 1), ('기르다', 1), ('바라다', 1), ('맛', 1), ('직원', 1), ('분들', 1), ('친절하다', 1), ('기념일', 1), ('분위기', 1), ('전반', 1), ('적', 1), ('으로', 1), ('짜다', 1), ('저', 1), ('는', 1), ('신경', 1), ('써다', 1), ('불쾌하다', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaK_i3zL2vO3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04cf6b1-910d-4db4-a974-dc0c4646fbee"
      },
      "source": [
        "w2i = {}\r\n",
        "for pair in tqdm(word_count):\r\n",
        "  if pair[0] not in w2i:\r\n",
        "    w2i[pair[0]] = len(w2i)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 60/60 [00:00<00:00, 238764.93it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiGqiEGDL5B_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9c3aeae-d483-4589-8610-f7f39954edda"
      },
      "source": [
        "print(train_tokenized)\r\n",
        "print(w2i)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['정말', '맛있다', '.', '추천', '하다', '.'], ['기대하다', '것', '보단', '별로', '이다', '.'], ['다', '좋다', '가격', '이', '너무', '비싸다', '다시', '가다', '싶다', '생각', '이', '안', '드네', '요', '.'], ['완전', '최고', '이다', '!', '재', '방문', '의사', '있다', '.'], ['음식', '도', '서비스', '도', '다', '만족스럽다', '.'], ['위생', '상태', '가', '좀', '별로', '이다', '.', '좀', '더', '개선', '되다', '기르다', '바라다', '.'], ['맛', '도', '좋다', '직원', '분들', '서비스', '도', '너무', '친절하다', '.'], ['기념일', '에', '방문', '하다', '음식', '도', '분위기', '도', '서비스', '도', '다', '좋다', '.'], ['전반', '적', '으로', '음식', '이', '너무', '짜다', '.', '저', '는', '별로', '이다', '.'], ['위생', '에', '조금', '더', '신경', '써다', '좋다', '.', '조금', '불쾌하다', '.']]\n",
            "{'.': 0, '도': 1, '이다': 2, '좋다': 3, '별로': 4, '다': 5, '이': 6, '너무': 7, '음식': 8, '서비스': 9, '하다': 10, '방문': 11, '위생': 12, '좀': 13, '더': 14, '에': 15, '조금': 16, '정말': 17, '맛있다': 18, '추천': 19, '기대하다': 20, '것': 21, '보단': 22, '가격': 23, '비싸다': 24, '다시': 25, '가다': 26, '싶다': 27, '생각': 28, '안': 29, '드네': 30, '요': 31, '완전': 32, '최고': 33, '!': 34, '재': 35, '의사': 36, '있다': 37, '만족스럽다': 38, '상태': 39, '가': 40, '개선': 41, '되다': 42, '기르다': 43, '바라다': 44, '맛': 45, '직원': 46, '분들': 47, '친절하다': 48, '기념일': 49, '분위기': 50, '전반': 51, '적': 52, '으로': 53, '짜다': 54, '저': 55, '는': 56, '신경': 57, '써다': 58, '불쾌하다': 59}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXA5zaPPM3Wd"
      },
      "source": [
        "실제 모델에 들어가기 위한 input을 만들기 위해 `Dataset` 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s47ssyVt89t1"
      },
      "source": [
        "class CBOWDataset(Dataset):\r\n",
        "  def __init__(self, train_tokenized, window_size=2):\r\n",
        "    self.x = []\r\n",
        "    self.y = []\r\n",
        "\r\n",
        "    for tokens in tqdm(train_tokenized):\r\n",
        "      token_ids = [w2i[token] for token in tokens]\r\n",
        "      for i, id in enumerate(token_ids):\r\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\r\n",
        "          self.x.append(token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\r\n",
        "          self.y.append(id)\r\n",
        "\r\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수, 2 * window_size)\r\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.x.shape[0]\r\n",
        "\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvInhQ33AMJv"
      },
      "source": [
        "class SkipGramDataset(Dataset):\r\n",
        "  def __init__(self, train_tokenized, window_size=2):\r\n",
        "    self.x = []\r\n",
        "    self.y = []\r\n",
        "\r\n",
        "    for tokens in tqdm(train_tokenized):\r\n",
        "      token_ids = [w2i[token] for token in tokens]\r\n",
        "      for i, id in enumerate(token_ids):\r\n",
        "        if i-window_size >= 0 and i+window_size < len(token_ids):\r\n",
        "          self.y += (token_ids[i-window_size:i] + token_ids[i+1:i+window_size+1])\r\n",
        "          self.x += [id] * 2 * window_size\r\n",
        "\r\n",
        "    self.x = torch.LongTensor(self.x)  # (전체 데이터 개수)\r\n",
        "    self.y = torch.LongTensor(self.y)  # (전체 데이터 개수)\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return self.x.shape[0]\r\n",
        "\r\n",
        "  def __getitem__(self, idx):\r\n",
        "    return self.x[idx], self.y[idx]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyAGV5IUUba0"
      },
      "source": [
        "각 모델에 맞는 `Dataset` 객체를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ep7Hm6oBWyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "111a5544-fafd-471a-e9a0-77459da0a1a8"
      },
      "source": [
        "cbow_set = CBOWDataset(train_tokenized)\r\n",
        "skipgram_set = SkipGramDataset(train_tokenized)\r\n",
        "print(list(skipgram_set))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 33879.68it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 4401.16it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[(tensor(0), tensor(17)), (tensor(0), tensor(18)), (tensor(0), tensor(19)), (tensor(0), tensor(10)), (tensor(19), tensor(18)), (tensor(19), tensor(0)), (tensor(19), tensor(10)), (tensor(19), tensor(0)), (tensor(22), tensor(20)), (tensor(22), tensor(21)), (tensor(22), tensor(4)), (tensor(22), tensor(2)), (tensor(4), tensor(21)), (tensor(4), tensor(22)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(23), tensor(5)), (tensor(23), tensor(3)), (tensor(23), tensor(6)), (tensor(23), tensor(7)), (tensor(6), tensor(3)), (tensor(6), tensor(23)), (tensor(6), tensor(7)), (tensor(6), tensor(24)), (tensor(7), tensor(23)), (tensor(7), tensor(6)), (tensor(7), tensor(24)), (tensor(7), tensor(25)), (tensor(24), tensor(6)), (tensor(24), tensor(7)), (tensor(24), tensor(25)), (tensor(24), tensor(26)), (tensor(25), tensor(7)), (tensor(25), tensor(24)), (tensor(25), tensor(26)), (tensor(25), tensor(27)), (tensor(26), tensor(24)), (tensor(26), tensor(25)), (tensor(26), tensor(27)), (tensor(26), tensor(28)), (tensor(27), tensor(25)), (tensor(27), tensor(26)), (tensor(27), tensor(28)), (tensor(27), tensor(6)), (tensor(28), tensor(26)), (tensor(28), tensor(27)), (tensor(28), tensor(6)), (tensor(28), tensor(29)), (tensor(6), tensor(27)), (tensor(6), tensor(28)), (tensor(6), tensor(29)), (tensor(6), tensor(30)), (tensor(29), tensor(28)), (tensor(29), tensor(6)), (tensor(29), tensor(30)), (tensor(29), tensor(31)), (tensor(30), tensor(6)), (tensor(30), tensor(29)), (tensor(30), tensor(31)), (tensor(30), tensor(0)), (tensor(2), tensor(32)), (tensor(2), tensor(33)), (tensor(2), tensor(34)), (tensor(2), tensor(35)), (tensor(34), tensor(33)), (tensor(34), tensor(2)), (tensor(34), tensor(35)), (tensor(34), tensor(11)), (tensor(35), tensor(2)), (tensor(35), tensor(34)), (tensor(35), tensor(11)), (tensor(35), tensor(36)), (tensor(11), tensor(34)), (tensor(11), tensor(35)), (tensor(11), tensor(36)), (tensor(11), tensor(37)), (tensor(36), tensor(35)), (tensor(36), tensor(11)), (tensor(36), tensor(37)), (tensor(36), tensor(0)), (tensor(9), tensor(8)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(38)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(38)), (tensor(5), tensor(0)), (tensor(40), tensor(12)), (tensor(40), tensor(39)), (tensor(40), tensor(13)), (tensor(40), tensor(4)), (tensor(13), tensor(39)), (tensor(13), tensor(40)), (tensor(13), tensor(4)), (tensor(13), tensor(2)), (tensor(4), tensor(40)), (tensor(4), tensor(13)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(2), tensor(13)), (tensor(2), tensor(4)), (tensor(2), tensor(0)), (tensor(2), tensor(13)), (tensor(0), tensor(4)), (tensor(0), tensor(2)), (tensor(0), tensor(13)), (tensor(0), tensor(14)), (tensor(13), tensor(2)), (tensor(13), tensor(0)), (tensor(13), tensor(14)), (tensor(13), tensor(41)), (tensor(14), tensor(0)), (tensor(14), tensor(13)), (tensor(14), tensor(41)), (tensor(14), tensor(42)), (tensor(41), tensor(13)), (tensor(41), tensor(14)), (tensor(41), tensor(42)), (tensor(41), tensor(43)), (tensor(42), tensor(14)), (tensor(42), tensor(41)), (tensor(42), tensor(43)), (tensor(42), tensor(44)), (tensor(43), tensor(41)), (tensor(43), tensor(42)), (tensor(43), tensor(44)), (tensor(43), tensor(0)), (tensor(3), tensor(45)), (tensor(3), tensor(1)), (tensor(3), tensor(46)), (tensor(3), tensor(47)), (tensor(46), tensor(1)), (tensor(46), tensor(3)), (tensor(46), tensor(47)), (tensor(46), tensor(9)), (tensor(47), tensor(3)), (tensor(47), tensor(46)), (tensor(47), tensor(9)), (tensor(47), tensor(1)), (tensor(9), tensor(46)), (tensor(9), tensor(47)), (tensor(9), tensor(1)), (tensor(9), tensor(7)), (tensor(1), tensor(47)), (tensor(1), tensor(9)), (tensor(1), tensor(7)), (tensor(1), tensor(48)), (tensor(7), tensor(9)), (tensor(7), tensor(1)), (tensor(7), tensor(48)), (tensor(7), tensor(0)), (tensor(11), tensor(49)), (tensor(11), tensor(15)), (tensor(11), tensor(10)), (tensor(11), tensor(8)), (tensor(10), tensor(15)), (tensor(10), tensor(11)), (tensor(10), tensor(8)), (tensor(10), tensor(1)), (tensor(8), tensor(11)), (tensor(8), tensor(10)), (tensor(8), tensor(1)), (tensor(8), tensor(50)), (tensor(1), tensor(10)), (tensor(1), tensor(8)), (tensor(1), tensor(50)), (tensor(1), tensor(1)), (tensor(50), tensor(8)), (tensor(50), tensor(1)), (tensor(50), tensor(1)), (tensor(50), tensor(9)), (tensor(1), tensor(1)), (tensor(1), tensor(50)), (tensor(1), tensor(9)), (tensor(1), tensor(1)), (tensor(9), tensor(50)), (tensor(9), tensor(1)), (tensor(9), tensor(1)), (tensor(9), tensor(5)), (tensor(1), tensor(1)), (tensor(1), tensor(9)), (tensor(1), tensor(5)), (tensor(1), tensor(3)), (tensor(5), tensor(9)), (tensor(5), tensor(1)), (tensor(5), tensor(3)), (tensor(5), tensor(0)), (tensor(53), tensor(51)), (tensor(53), tensor(52)), (tensor(53), tensor(8)), (tensor(53), tensor(6)), (tensor(8), tensor(52)), (tensor(8), tensor(53)), (tensor(8), tensor(6)), (tensor(8), tensor(7)), (tensor(6), tensor(53)), (tensor(6), tensor(8)), (tensor(6), tensor(7)), (tensor(6), tensor(54)), (tensor(7), tensor(8)), (tensor(7), tensor(6)), (tensor(7), tensor(54)), (tensor(7), tensor(0)), (tensor(54), tensor(6)), (tensor(54), tensor(7)), (tensor(54), tensor(0)), (tensor(54), tensor(55)), (tensor(0), tensor(7)), (tensor(0), tensor(54)), (tensor(0), tensor(55)), (tensor(0), tensor(56)), (tensor(55), tensor(54)), (tensor(55), tensor(0)), (tensor(55), tensor(56)), (tensor(55), tensor(4)), (tensor(56), tensor(0)), (tensor(56), tensor(55)), (tensor(56), tensor(4)), (tensor(56), tensor(2)), (tensor(4), tensor(55)), (tensor(4), tensor(56)), (tensor(4), tensor(2)), (tensor(4), tensor(0)), (tensor(16), tensor(12)), (tensor(16), tensor(15)), (tensor(16), tensor(14)), (tensor(16), tensor(57)), (tensor(14), tensor(15)), (tensor(14), tensor(16)), (tensor(14), tensor(57)), (tensor(14), tensor(58)), (tensor(57), tensor(16)), (tensor(57), tensor(14)), (tensor(57), tensor(58)), (tensor(57), tensor(3)), (tensor(58), tensor(14)), (tensor(58), tensor(57)), (tensor(58), tensor(3)), (tensor(58), tensor(0)), (tensor(3), tensor(57)), (tensor(3), tensor(58)), (tensor(3), tensor(0)), (tensor(3), tensor(16)), (tensor(0), tensor(58)), (tensor(0), tensor(3)), (tensor(0), tensor(16)), (tensor(0), tensor(59)), (tensor(16), tensor(3)), (tensor(16), tensor(0)), (tensor(16), tensor(59)), (tensor(16), tensor(0))]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QSo73PoRyd9"
      },
      "source": [
        "### **모델 Class 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnnk44R6R28x"
      },
      "source": [
        "차례대로 두 가지 Word2Vec 모델을 구현합니다.  \r\n",
        "\r\n",
        "\r\n",
        "*   `self.embedding`: `vocab_size` 크기의 one-hot vector를 특정 크기의 `dim` 차원으로 embedding 시키는 layer.\r\n",
        "*   `self.linear`: 변환된 embedding vector를 다시 원래 `vocab_size`로 바꾸는 layer.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_HP1ISq5CWv"
      },
      "source": [
        "class CBOW(nn.Module):\r\n",
        "  def __init__(self, vocab_size, dim):\r\n",
        "    super(CBOW, self).__init__()\r\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n",
        "    self.linear = nn.Linear(dim, vocab_size)\r\n",
        "\r\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\r\n",
        "  def forward(self, x):  # x: (B, 2W)\r\n",
        "    embeddings = self.embedding(x)  # (B, 2W, d_w)\r\n",
        "    embeddings = torch.sum(embeddings, dim=1)  # (B, d_w)\r\n",
        "    output = self.linear(embeddings)  # (B, V)\r\n",
        "    return output"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQAUApww68MJ"
      },
      "source": [
        "class SkipGram(nn.Module):\r\n",
        "  def __init__(self, vocab_size, dim):\r\n",
        "    super(SkipGram, self).__init__()\r\n",
        "    self.embedding = nn.Embedding(vocab_size, dim, sparse=True)\r\n",
        "    self.linear = nn.Linear(dim, vocab_size)\r\n",
        "\r\n",
        "  # B: batch size, W: window size, d_w: word embedding size, V: vocab size\r\n",
        "  def forward(self, x): # x: (B)\r\n",
        "    embeddings = self.embedding(x)  # (B, d_w)\r\n",
        "    output = self.linear(embeddings)  # (B, V)\r\n",
        "    return output"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58cJalkDWYMT"
      },
      "source": [
        "두 가지 모델을 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWUXEi8WeM-"
      },
      "source": [
        "cbow = CBOW(vocab_size=len(w2i), dim=256)\r\n",
        "skipgram = SkipGram(vocab_size=len(w2i), dim=256)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxP7qdtNWil1"
      },
      "source": [
        "### **모델 학습**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVggZrQ4WpBS"
      },
      "source": [
        "다음과 같이 hyperparamter를 세팅하고 `DataLoader` 객체를 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygVdz5rSBeNu"
      },
      "source": [
        "batch_size=4\r\n",
        "learning_rate = 5e-4\r\n",
        "num_epochs = 5\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "\r\n",
        "cbow_loader = DataLoader(cbow_set, batch_size=batch_size)\r\n",
        "skipgram_loader = DataLoader(skipgram_set, batch_size=batch_size)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekixqKB3X5C1"
      },
      "source": [
        "첫번째로 CBOW 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d95qR7oC822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "365cb4d1-2cc8-4a15-f39c-e12f6a1170e1"
      },
      "source": [
        "cbow.train()\r\n",
        "cbow = cbow.to(device)\r\n",
        "optim = torch.optim.SGD(cbow.parameters(), lr=learning_rate)\r\n",
        "loss_function = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "for e in range(1, num_epochs+1):\r\n",
        "  print(\"#\" * 50)\r\n",
        "  print(f\"Epoch: {e}\")\r\n",
        "  for batch in tqdm(cbow_loader):\r\n",
        "    x, y = batch\r\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n",
        "    output = cbow(x)  # (B, V)\r\n",
        " \r\n",
        "    optim.zero_grad()\r\n",
        "    loss = loss_function(output, y)\r\n",
        "    loss.backward()\r\n",
        "    optim.step()\r\n",
        "\r\n",
        "    print(f\"Train loss: {loss.item()}\")\r\n",
        "\r\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 130.09it/s]\n",
            "100%|██████████| 16/16 [00:00<00:00, 698.11it/s]\n",
            "100%|██████████| 16/16 [00:00<00:00, 677.14it/s]\n",
            "  0%|          | 0/16 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "##################################################\n",
            "Epoch: 1\n",
            "Train loss: 4.80348014831543\n",
            "Train loss: 4.353360176086426\n",
            "Train loss: 5.988618850708008\n",
            "Train loss: 4.567574501037598\n",
            "Train loss: 5.384766578674316\n",
            "Train loss: 4.348655700683594\n",
            "Train loss: 5.378296375274658\n",
            "Train loss: 5.190069198608398\n",
            "Train loss: 4.461023330688477\n",
            "Train loss: 4.014639854431152\n",
            "Train loss: 3.912533760070801\n",
            "Train loss: 3.9667863845825195\n",
            "Train loss: 4.599996566772461\n",
            "Train loss: 5.2390947341918945\n",
            "Train loss: 4.460725784301758\n",
            "Train loss: 5.714531898498535\n",
            "##################################################\n",
            "Epoch: 2\n",
            "Train loss: 4.6101861000061035\n",
            "Train loss: 4.210658073425293\n",
            "Train loss: 5.827690124511719\n",
            "Train loss: 4.439815521240234\n",
            "Train loss: 5.252415657043457\n",
            "Train loss: 4.071657657623291\n",
            "Train loss: 5.203367233276367\n",
            "Train loss: 5.058775901794434\n",
            "Train loss: 4.344847679138184\n",
            "Train loss: 3.8284854888916016\n",
            "Train loss: 3.748732805252075\n",
            "Train loss: 3.5951831340789795\n",
            "Train loss: 4.458020210266113\n",
            "Train loss: 5.122245788574219\n",
            "Train loss: 4.319595813751221\n",
            "Train loss: 5.571252822875977\n",
            "##################################################\n",
            "Epoch: 3\n",
            "Train loss: 4.423433780670166\n",
            "Train loss: 4.0713043212890625\n",
            "Train loss: 5.669358253479004\n",
            "Train loss: 4.313726425170898\n",
            "Train loss: 5.121541976928711\n",
            "Train loss: 3.8101658821105957\n",
            "Train loss: 5.031087398529053\n",
            "Train loss: 4.929318428039551\n",
            "Train loss: 4.2349772453308105\n",
            "Train loss: 3.6500473022460938\n",
            "Train loss: 3.5956780910491943\n",
            "Train loss: 3.25079345703125\n",
            "Train loss: 4.318302154541016\n",
            "Train loss: 5.007801055908203\n",
            "Train loss: 4.180412292480469\n",
            "Train loss: 5.431035995483398\n",
            "##################################################\n",
            "Epoch: 4\n",
            "Train loss: 4.24271297454834\n",
            "Train loss: 3.935276985168457\n",
            "Train loss: 5.5135297775268555\n",
            "Train loss: 4.189335346221924\n",
            "Train loss: 4.992161750793457\n",
            "Train loss: 3.566498279571533\n",
            "Train loss: 4.861435413360596\n",
            "Train loss: 4.801640510559082\n",
            "Train loss: 4.131167888641357\n",
            "Train loss: 3.4802329540252686\n",
            "Train loss: 3.45430850982666\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:00<00:00, 586.64it/s]\n",
            "100%|██████████| 16/16 [00:00<00:00, 614.66it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 2.937896490097046\n",
            "Train loss: 4.180788516998291\n",
            "Train loss: 4.8955488204956055\n",
            "Train loss: 4.043193817138672\n",
            "Train loss: 5.293586254119873\n",
            "##################################################\n",
            "Epoch: 5\n",
            "Train loss: 4.067689895629883\n",
            "Train loss: 3.80252742767334\n",
            "Train loss: 5.360133647918701\n",
            "Train loss: 4.066681861877441\n",
            "Train loss: 4.864297389984131\n",
            "Train loss: 3.3425750732421875\n",
            "Train loss: 4.69443941116333\n",
            "Train loss: 4.675711154937744\n",
            "Train loss: 4.033030033111572\n",
            "Train loss: 3.3198599815368652\n",
            "Train loss: 3.3251118659973145\n",
            "Train loss: 2.6598780155181885\n",
            "Train loss: 4.045436382293701\n",
            "Train loss: 4.785314559936523\n",
            "Train loss: 3.907994270324707\n",
            "Train loss: 5.158686637878418\n",
            "Finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDahBf6IX4py"
      },
      "source": [
        "다음으로 Skip-gram 모델 학습입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJxGEusqFV5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f9aade-5490-48aa-ddb7-18ce86b71a3e"
      },
      "source": [
        "skipgram.train()\r\n",
        "skipgram = skipgram.to(device)\r\n",
        "optim = torch.optim.SGD(skipgram.parameters(), lr=learning_rate)\r\n",
        "loss_function = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "for e in range(1, num_epochs+1):\r\n",
        "  print(\"#\" * 50)\r\n",
        "  print(f\"Epoch: {e}\")\r\n",
        "  for batch in tqdm(skipgram_loader):\r\n",
        "    x, y = batch\r\n",
        "    x, y = x.to(device), y.to(device) # (B, W), (B)\r\n",
        "    output = skipgram(x)  # (B, V)\r\n",
        "\r\n",
        "    optim.zero_grad()\r\n",
        "    loss = loss_function(output, y)\r\n",
        "    loss.backward()\r\n",
        "    optim.step()\r\n",
        "\r\n",
        "    print(f\"Train loss: {loss.item()}\")\r\n",
        "\r\n",
        "print(\"Finished.\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 760.51it/s]\n",
            "100%|██████████| 64/64 [00:00<00:00, 780.70it/s]\n",
            "  0%|          | 0/64 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "##################################################\n",
            "Epoch: 1\n",
            "Train loss: 3.7816238403320312\n",
            "Train loss: 4.267120361328125\n",
            "Train loss: 4.215713024139404\n",
            "Train loss: 4.752714157104492\n",
            "Train loss: 4.187987327575684\n",
            "Train loss: 4.019939422607422\n",
            "Train loss: 4.580994129180908\n",
            "Train loss: 4.206258773803711\n",
            "Train loss: 4.453839302062988\n",
            "Train loss: 4.290581703186035\n",
            "Train loss: 3.7114858627319336\n",
            "Train loss: 4.110230922698975\n",
            "Train loss: 4.023531913757324\n",
            "Train loss: 4.344503879547119\n",
            "Train loss: 3.731412410736084\n",
            "Train loss: 4.101868629455566\n",
            "Train loss: 3.892960786819458\n",
            "Train loss: 4.536526203155518\n",
            "Train loss: 4.085565567016602\n",
            "Train loss: 3.7526304721832275\n",
            "Train loss: 4.616669654846191\n",
            "Train loss: 3.783750057220459\n",
            "Train loss: 4.68772554397583\n",
            "Train loss: 4.172626972198486\n",
            "Train loss: 3.9627764225006104\n",
            "Train loss: 4.725791931152344\n",
            "Train loss: 4.18694543838501\n",
            "Train loss: 4.426153182983398\n",
            "Train loss: 4.216753959655762\n",
            "Train loss: 3.9124090671539307\n",
            "Train loss: 4.282020092010498\n",
            "Train loss: 4.5727858543396\n",
            "Train loss: 4.244597911834717\n",
            "Train loss: 4.419821739196777\n",
            "Train loss: 4.275176525115967\n",
            "Train loss: 4.204411506652832\n",
            "Train loss: 4.359814643859863\n",
            "Train loss: 4.134059429168701\n",
            "Train loss: 4.121730327606201\n",
            "Train loss: 4.2646098136901855\n",
            "Train loss: 3.9292826652526855\n",
            "Train loss: 4.332383632659912\n",
            "Train loss: 4.038943767547607\n",
            "Train loss: 4.223748207092285\n",
            "Train loss: 4.174892425537109\n",
            "Train loss: 4.377224922180176\n",
            "Train loss: 4.2689714431762695\n",
            "Train loss: 4.474923610687256\n",
            "Train loss: 4.054044723510742\n",
            "Train loss: 3.722611904144287\n",
            "Train loss: 3.940957546234131\n",
            "Train loss: 4.592228412628174\n",
            "Train loss: 4.319056510925293\n",
            "Train loss: 4.139887809753418\n",
            "Train loss: 4.162934303283691\n",
            "Train loss: 3.955209732055664\n",
            "Train loss: 4.358244895935059\n",
            "Train loss: 4.332576751708984\n",
            "Train loss: 4.457947731018066\n",
            "Train loss: 4.7588677406311035\n",
            "Train loss: 3.6744649410247803\n",
            "Train loss: 4.230642795562744\n",
            "Train loss: 3.8347573280334473\n",
            "Train loss: 4.464559078216553\n",
            "##################################################\n",
            "Epoch: 2\n",
            "Train loss: 3.7613842487335205\n",
            "Train loss: 4.221484184265137\n",
            "Train loss: 4.183317184448242\n",
            "Train loss: 4.6841511726379395\n",
            "Train loss: 4.149789810180664\n",
            "Train loss: 3.9824228286743164\n",
            "Train loss: 4.535680770874023\n",
            "Train loss: 4.172688007354736\n",
            "Train loss: 4.418055534362793\n",
            "Train loss: 4.259597301483154\n",
            "Train loss: 3.68501353263855\n",
            "Train loss: 4.064275741577148\n",
            "Train loss: 3.9986047744750977\n",
            "Train loss: 4.313922882080078\n",
            "Train loss: 3.698242425918579\n",
            "Train loss: 4.067687034606934\n",
            "Train loss: 3.868373394012451\n",
            "Train loss: 4.504955291748047\n",
            "Train loss: 4.064152717590332\n",
            "Train loss: 3.7209153175354004\n",
            "Train loss: 4.501636505126953\n",
            "Train loss: 3.707759380340576\n",
            "Train loss: 4.630829334259033\n",
            "Train loss: 4.142464637756348\n",
            "Train loss: 3.9292867183685303\n",
            "Train loss: 4.65590763092041\n",
            "Train loss: 4.132647514343262\n",
            "Train loss: 4.396361351013184\n",
            "Train loss: 4.179806709289551\n",
            "Train loss: 3.884827136993408\n",
            "Train loss: 4.254117965698242\n",
            "Train loss: 4.541129112243652\n",
            "Train loss: 4.21928596496582\n",
            "Train loss: 4.397524833679199\n",
            "Train loss: 4.251161575317383\n",
            "Train loss: 4.172460079193115\n",
            "Train loss: 4.2915520668029785\n",
            "Train loss: 4.093662261962891\n",
            "Train loss: 4.0768723487854\n",
            "Train loss: 4.23836612701416\n",
            "Train loss: 3.910738229751587\n",
            "Train loss: 4.311431884765625\n",
            "Train loss: 3.986027717590332\n",
            "Train loss: 4.17980432510376\n",
            "Train loss: 4.0670647621154785\n",
            "Train loss: 4.2640061378479\n",
            "Train loss: 4.192063331604004\n",
            "Train loss: 4.419229984283447\n",
            "Train loss: 4.020662307739258\n",
            "Train loss: 3.6962687969207764\n",
            "Train loss: 3.9034643173217773\n",
            "Train loss: 4.538569927215576\n",
            "Train loss: 4.288881301879883\n",
            "Train loss: 4.110035419464111\n",
            "Train loss: 4.128586769104004\n",
            "Train loss: 3.9244179725646973\n",
            "Train loss: 4.289837837219238\n",
            "Train loss: 4.30943489074707\n",
            "Train loss: 4.435641765594482\n",
            "Train loss: 4.717435836791992\n",
            "Train loss: 3.641511917114258\n",
            "Train loss: 4.201787948608398\n",
            "Train loss: 3.811312198638916\n",
            "Train loss: 4.425105571746826\n",
            "##################################################\n",
            "Epoch: 3\n",
            "Train loss: 3.7417421340942383\n",
            "Train loss: 4.176042556762695\n",
            "Train loss: 4.1511149406433105\n",
            "Train loss: 4.616270065307617\n",
            "Train loss: 4.111921310424805\n",
            "Train loss: 3.9455554485321045\n",
            "Train loss: 4.490966796875\n",
            "Train loss: 4.139344215393066\n",
            "Train loss: 4.382518768310547\n",
            "Train loss: 4.2288031578063965\n",
            "Train loss: 3.6587212085723877\n",
            "Train loss: 4.0187506675720215\n",
            "Train loss: 3.974213123321533\n",
            "Train loss: 4.2834954261779785\n",
            "Train loss: 3.6653642654418945\n",
            "Train loss: 4.0340728759765625\n",
            "Train loss: 3.843930244445801\n",
            "Train loss: 4.473488807678223\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 64/64 [00:00<00:00, 743.97it/s]\n",
            "100%|██████████| 64/64 [00:00<00:00, 761.32it/s]\n",
            "  0%|          | 0/64 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 4.042933940887451\n",
            "Train loss: 3.689474582672119\n",
            "Train loss: 4.388166427612305\n",
            "Train loss: 3.6338419914245605\n",
            "Train loss: 4.574385643005371\n",
            "Train loss: 4.1124749183654785\n",
            "Train loss: 3.8962178230285645\n",
            "Train loss: 4.586732864379883\n",
            "Train loss: 4.079136371612549\n",
            "Train loss: 4.367153167724609\n",
            "Train loss: 4.143271446228027\n",
            "Train loss: 3.857530355453491\n",
            "Train loss: 4.226344585418701\n",
            "Train loss: 4.509611129760742\n",
            "Train loss: 4.194073677062988\n",
            "Train loss: 4.37539005279541\n",
            "Train loss: 4.227226257324219\n",
            "Train loss: 4.140751838684082\n",
            "Train loss: 4.224636077880859\n",
            "Train loss: 4.0549116134643555\n",
            "Train loss: 4.032726764678955\n",
            "Train loss: 4.212347030639648\n",
            "Train loss: 3.8922855854034424\n",
            "Train loss: 4.290648460388184\n",
            "Train loss: 3.9344756603240967\n",
            "Train loss: 4.136038780212402\n",
            "Train loss: 3.961487293243408\n",
            "Train loss: 4.152508735656738\n",
            "Train loss: 4.1172404289245605\n",
            "Train loss: 4.364009380340576\n",
            "Train loss: 3.9875025749206543\n",
            "Train loss: 3.670264720916748\n",
            "Train loss: 3.866649866104126\n",
            "Train loss: 4.485537528991699\n",
            "Train loss: 4.258846282958984\n",
            "Train loss: 4.080878257751465\n",
            "Train loss: 4.094571590423584\n",
            "Train loss: 3.8938608169555664\n",
            "Train loss: 4.222217559814453\n",
            "Train loss: 4.2864789962768555\n",
            "Train loss: 4.413506507873535\n",
            "Train loss: 4.676196098327637\n",
            "Train loss: 3.608872890472412\n",
            "Train loss: 4.1731977462768555\n",
            "Train loss: 3.7884788513183594\n",
            "Train loss: 4.385890483856201\n",
            "##################################################\n",
            "Epoch: 4\n",
            "Train loss: 3.7226781845092773\n",
            "Train loss: 4.130802154541016\n",
            "Train loss: 4.1191086769104\n",
            "Train loss: 4.549108505249023\n",
            "Train loss: 4.074387073516846\n",
            "Train loss: 3.9093446731567383\n",
            "Train loss: 4.446864128112793\n",
            "Train loss: 4.106231212615967\n",
            "Train loss: 4.347231388092041\n",
            "Train loss: 4.198200702667236\n",
            "Train loss: 3.6326117515563965\n",
            "Train loss: 3.973660707473755\n",
            "Train loss: 3.9503543376922607\n",
            "Train loss: 4.253223896026611\n",
            "Train loss: 3.632781505584717\n",
            "Train loss: 4.001033782958984\n",
            "Train loss: 3.8196306228637695\n",
            "Train loss: 4.442130088806152\n",
            "Train loss: 4.021908760070801\n",
            "Train loss: 3.658310890197754\n",
            "Train loss: 4.276419162750244\n",
            "Train loss: 3.5620880126953125\n",
            "Train loss: 4.518414497375488\n",
            "Train loss: 4.082662582397461\n",
            "Train loss: 3.863572120666504\n",
            "Train loss: 4.518306732177734\n",
            "Train loss: 4.026440143585205\n",
            "Train loss: 4.338517665863037\n",
            "Train loss: 4.107152938842773\n",
            "Train loss: 3.830522298812866\n",
            "Train loss: 4.198701858520508\n",
            "Train loss: 4.47823429107666\n",
            "Train loss: 4.168961048126221\n",
            "Train loss: 4.353409767150879\n",
            "Train loss: 4.203371524810791\n",
            "Train loss: 4.109292030334473\n",
            "Train loss: 4.159180641174316\n",
            "Train loss: 4.017862319946289\n",
            "Train loss: 3.9893124103546143\n",
            "Train loss: 4.186551570892334\n",
            "Train loss: 3.873918056488037\n",
            "Train loss: 4.270026206970215\n",
            "Train loss: 3.8843350410461426\n",
            "Train loss: 4.092452049255371\n",
            "Train loss: 3.8582892417907715\n",
            "Train loss: 4.042905807495117\n",
            "Train loss: 4.044593811035156\n",
            "Train loss: 4.30928373336792\n",
            "Train loss: 3.95456862449646\n",
            "Train loss: 3.644601345062256\n",
            "Train loss: 3.8305225372314453\n",
            "Train loss: 4.433152198791504\n",
            "Train loss: 4.2289533615112305\n",
            "Train loss: 4.052402496337891\n",
            "Train loss: 4.060887336730957\n",
            "Train loss: 3.8635432720184326\n",
            "Train loss: 4.155424118041992\n",
            "Train loss: 4.263710975646973\n",
            "Train loss: 4.391541004180908\n",
            "Train loss: 4.6351518630981445\n",
            "Train loss: 3.576551914215088\n",
            "Train loss: 4.144876480102539\n",
            "Train loss: 3.766240119934082\n",
            "Train loss: 4.346920013427734\n",
            "##################################################\n",
            "Epoch: 5\n",
            "Train loss: 3.7041749954223633\n",
            "Train loss: 4.085770130157471\n",
            "Train loss: 4.0873003005981445\n",
            "Train loss: 4.482707977294922\n",
            "Train loss: 4.037190914154053\n",
            "Train loss: 3.873797655105591\n",
            "Train loss: 4.403385639190674\n",
            "Train loss: 4.073352336883545\n",
            "Train loss: 4.312195301055908\n",
            "Train loss: 4.167791366577148\n",
            "Train loss: 3.606686592102051\n",
            "Train loss: 3.9290122985839844\n",
            "Train loss: 3.927025318145752\n",
            "Train loss: 4.223109245300293\n",
            "Train loss: 3.6004958152770996\n",
            "Train loss: 3.9685792922973633\n",
            "Train loss: 3.7954747676849365\n",
            "Train loss: 4.410881519317627\n",
            "Train loss: 4.001078128814697\n",
            "Train loss: 3.6274261474609375\n",
            "Train loss: 4.166581153869629\n",
            "Train loss: 3.492586135864258\n",
            "Train loss: 4.462936878204346\n",
            "Train loss: 4.0530290603637695\n",
            "Train loss: 3.831350088119507\n",
            "Train loss: 4.450672626495361\n",
            "Train loss: 3.9745893478393555\n",
            "Train loss: 4.3104424476623535\n",
            "Train loss: 4.0714521408081055\n",
            "Train loss: 3.8038055896759033\n",
            "Train loss: 4.17119026184082\n",
            "Train loss: 4.4469990730285645\n",
            "Train loss: 4.143949508666992\n",
            "Train loss: 4.331575870513916\n",
            "Train loss: 4.179598808288574\n",
            "Train loss: 4.078085422515869\n",
            "Train loss: 4.09531307220459\n",
            "Train loss: 3.982560634613037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 64/64 [00:00<00:00, 744.54it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 3.9466466903686523\n",
            "Train loss: 4.160982131958008\n",
            "Train loss: 3.8556268215179443\n",
            "Train loss: 4.249560832977295\n",
            "Train loss: 3.8356518745422363\n",
            "Train loss: 4.049046516418457\n",
            "Train loss: 3.7576000690460205\n",
            "Train loss: 3.935391426086426\n",
            "Train loss: 3.974210500717163\n",
            "Train loss: 4.255073070526123\n",
            "Train loss: 3.921862840652466\n",
            "Train loss: 3.619279623031616\n",
            "Train loss: 3.795088768005371\n",
            "Train loss: 4.381435394287109\n",
            "Train loss: 4.199204444885254\n",
            "Train loss: 4.024593830108643\n",
            "Train loss: 4.02753210067749\n",
            "Train loss: 3.8334665298461914\n",
            "Train loss: 4.08950138092041\n",
            "Train loss: 4.241134166717529\n",
            "Train loss: 4.369747638702393\n",
            "Train loss: 4.594306468963623\n",
            "Train loss: 3.5445523262023926\n",
            "Train loss: 4.116828918457031\n",
            "Train loss: 3.7445807456970215\n",
            "Train loss: 4.30820369720459\n",
            "Finished.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi0sbHV6dEOR"
      },
      "source": [
        "### **테스트**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGarLWxXeJvz"
      },
      "source": [
        "학습된 각 모델을 이용하여 test 단어들의 word embedding을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A1wrl-L_RjF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45c57f25-151a-4da5-c937-1583777588d4"
      },
      "source": [
        "for word in test_words:\r\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\r\n",
        "  emb = cbow.embedding(input_id)\r\n",
        "\r\n",
        "  print(f\"Word: {word}\")\r\n",
        "  print(emb.squeeze(0))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word: 음식\n",
            "tensor([-3.8133e-01,  1.5533e-01,  1.4973e+00,  2.2798e+00,  7.4904e-02,\n",
            "         1.4004e+00,  2.5119e-01, -1.7012e+00, -4.3473e-01, -3.5296e-01,\n",
            "         4.9321e-01, -9.9620e-01, -6.9636e-01,  9.5526e-01, -2.5815e-04,\n",
            "        -8.0536e-01,  9.9559e-01, -2.0495e+00, -1.3111e+00, -1.5635e-01,\n",
            "         7.6210e-01, -7.4923e-01, -4.8993e-01, -6.7755e-01,  7.3313e-01,\n",
            "        -1.8821e-01,  7.8118e-01, -1.0952e+00, -4.2046e-01, -1.7938e+00,\n",
            "         1.1852e-01, -1.9163e-01, -2.6665e-01, -2.0962e-01,  9.5891e-01,\n",
            "         7.8954e-02, -5.7245e-01,  9.6510e-02, -1.4133e+00, -4.5091e-01,\n",
            "         5.6576e-01,  1.1070e+00,  2.0158e+00,  8.6939e-03, -4.9678e-01,\n",
            "         5.7098e-01, -8.4483e-02,  2.5549e-02,  1.2124e+00,  2.4845e-01,\n",
            "        -4.6574e-01,  4.3416e-02,  7.1051e-01,  8.2970e-01,  7.2512e-01,\n",
            "        -1.6431e+00,  1.1731e+00, -6.4687e-01,  1.6348e+00, -8.4508e-01,\n",
            "         2.6567e-01,  3.5450e-01,  3.8265e-01, -5.5076e-01,  8.2978e-01,\n",
            "        -6.9080e-01,  9.1097e-01, -4.7613e-02, -5.8446e-01,  7.4573e-01,\n",
            "        -1.0239e+00, -1.1957e+00,  4.1043e-01,  2.5067e-01,  1.8997e-01,\n",
            "         8.5497e-01,  7.2319e-01,  1.5179e+00, -3.6687e-01,  2.7194e-01,\n",
            "        -7.8714e-01,  1.3256e+00,  1.2944e+00,  9.5263e-01, -1.1177e+00,\n",
            "        -2.0053e+00,  1.9501e+00, -2.0281e-01,  1.3341e+00,  4.0732e-01,\n",
            "         1.8461e+00,  2.2489e+00,  8.7099e-01, -1.4958e-01,  3.5892e-02,\n",
            "         2.8312e-01, -1.1616e+00, -4.8517e-01, -2.4955e-01,  1.1171e+00,\n",
            "        -1.6793e+00, -3.1921e-01,  1.2023e+00,  1.5777e+00, -4.0841e-01,\n",
            "         1.6407e-02, -6.2932e-01, -3.2966e+00,  3.7056e-01,  9.2919e-01,\n",
            "        -1.0995e-01,  6.7732e-01,  2.9160e-01, -3.1545e-01, -6.5363e-01,\n",
            "         1.4909e+00,  1.1397e+00,  8.3226e-01, -3.8738e-02,  1.9479e+00,\n",
            "         4.2145e-01, -1.0005e-01,  2.1071e-01, -1.1529e-01, -5.9123e-03,\n",
            "        -1.3033e+00, -3.3391e-01,  1.2848e+00, -7.5706e-01, -1.3227e+00,\n",
            "        -9.6189e-01,  1.0076e+00, -1.3840e+00, -1.7642e+00, -1.4879e+00,\n",
            "         6.8532e-01,  1.5381e+00, -3.6411e-01,  3.4717e-01, -5.0614e-02,\n",
            "         1.6980e-01,  2.9599e+00,  5.2163e-01,  6.7088e-01,  9.4364e-01,\n",
            "        -1.2976e-01, -3.7285e-02,  1.2784e+00,  1.2446e+00, -2.2954e+00,\n",
            "         5.6735e-01,  2.3550e-01,  3.0292e-01,  2.8333e-01, -2.5635e-01,\n",
            "         4.0411e-01, -9.0145e-01,  3.6126e-01,  8.8978e-01, -6.3392e-01,\n",
            "         8.5790e-01, -3.4781e-01,  2.3548e-01,  9.2983e-01, -1.1331e-01,\n",
            "        -7.0287e-01, -2.3145e-01,  4.9703e-02,  9.5929e-01,  2.0107e+00,\n",
            "        -9.5688e-01,  8.8378e-01, -2.8486e+00, -4.3765e-01, -6.3785e-01,\n",
            "         1.0593e+00,  2.3185e-01,  5.1249e-01, -1.6562e+00, -3.3344e+00,\n",
            "         4.2309e-01,  5.8298e-01,  1.4752e-01, -5.2974e-01, -1.7156e+00,\n",
            "        -1.0069e-01, -1.3815e+00,  3.7025e-01,  1.2867e-01, -5.6170e-01,\n",
            "         7.3484e-01,  9.7299e-01,  1.4457e+00,  7.4760e-01,  9.2857e-01,\n",
            "        -1.0958e+00, -6.0203e-01, -2.3771e-01, -7.7337e-01, -3.0707e-02,\n",
            "         5.2388e-01,  1.0642e+00, -1.5586e+00, -1.1467e+00,  3.3561e-01,\n",
            "         3.7412e-01, -5.9334e-01,  7.3433e-01,  2.4000e-02,  8.4388e-01,\n",
            "        -1.2713e+00,  1.1577e+00, -7.4372e-01, -4.6163e-01,  9.2541e-03,\n",
            "         7.5771e-02,  4.1543e-01, -7.1751e-01,  9.2777e-01, -4.0681e-01,\n",
            "        -1.9988e+00, -2.0772e-01,  1.1148e-01, -7.5188e-01, -3.6547e-01,\n",
            "         5.0693e-01, -1.0114e+00,  3.3727e-01, -9.6050e-01, -5.2928e-01,\n",
            "         9.1155e-01,  2.0449e-01, -4.7091e-01,  2.4220e-02,  6.1874e-01,\n",
            "         1.4195e+00,  4.4505e-01, -2.9300e-01,  4.2669e-02,  2.6371e+00,\n",
            "         5.9379e-01, -7.2882e-01, -1.1391e-01, -6.4067e-01, -1.0500e+00,\n",
            "        -5.4884e-01, -3.7153e-01,  2.3761e+00, -3.6307e-01, -1.2142e+00,\n",
            "         6.5385e-01,  2.1408e+00,  1.9379e+00, -1.0335e+00, -7.4611e-01,\n",
            "         7.8897e-01], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 맛\n",
            "tensor([-2.5030e-01,  1.1398e+00,  5.3632e-01, -2.7862e-01,  3.4080e-01,\n",
            "         1.4287e+00,  2.4160e-01, -7.9164e-01, -3.4242e-02,  1.0951e+00,\n",
            "        -2.4192e-02,  8.2996e-01,  7.6829e-01, -9.7323e-02, -3.2027e-01,\n",
            "         4.2838e-01, -5.0988e-02, -3.6723e+00, -4.4323e-01, -5.1753e-01,\n",
            "        -1.8141e-01, -1.3660e+00, -6.9923e-01, -2.8425e-01,  9.0902e-01,\n",
            "         1.7404e+00, -5.2167e-01, -1.6785e+00, -1.0215e+00,  8.4426e-01,\n",
            "         1.9607e+00,  1.1682e+00, -1.9222e-02,  1.8385e+00,  2.0311e+00,\n",
            "        -2.0131e-01, -1.5166e+00, -5.9066e-01, -8.2432e-01,  1.8027e+00,\n",
            "         4.6213e-01,  1.5811e+00,  8.5756e-01,  3.6287e-01, -1.0993e+00,\n",
            "        -3.8709e-01,  1.1696e-01, -1.7643e+00,  3.6608e+00,  1.4975e-01,\n",
            "         9.0739e-01, -6.6191e-01,  7.7779e-01, -1.1230e+00,  9.7259e-01,\n",
            "         3.7159e-01,  3.2446e-01, -7.2653e-01, -1.4390e-01, -9.7625e-01,\n",
            "        -1.3404e+00,  1.0004e+00, -1.4191e-01,  5.2561e-01, -1.6377e-01,\n",
            "         7.7861e-01,  9.0396e-01, -2.4875e-01, -5.9972e-01,  3.2799e-01,\n",
            "         4.6132e-01, -7.7890e-02, -8.1209e-01, -1.9454e-02, -1.6000e+00,\n",
            "         8.1901e-01,  1.9166e+00, -5.9535e-01,  1.8993e+00,  5.7094e-01,\n",
            "         8.7961e-02,  2.5831e-01, -5.2552e-01, -1.8489e-01,  2.6885e+00,\n",
            "        -1.0156e+00,  4.5868e-01, -2.3072e-01,  2.1086e+00,  6.6199e-01,\n",
            "        -1.1662e-01, -1.1809e+00,  6.4615e-01, -1.8910e+00,  8.8347e-01,\n",
            "         8.9943e-01, -9.7427e-01,  1.2365e+00, -1.7256e+00, -7.4151e-01,\n",
            "        -1.2613e+00, -1.3387e+00,  4.5775e-01, -5.0903e-01, -4.4413e-01,\n",
            "         2.9632e-01, -8.6613e-01,  2.1778e+00,  1.0620e+00, -2.4431e-01,\n",
            "        -7.6914e-01, -1.3485e+00, -2.1572e+00,  2.9970e-01, -2.2004e+00,\n",
            "        -5.0607e-01,  8.3133e-02,  2.5741e-01,  2.3977e+00,  1.5298e+00,\n",
            "        -1.4595e+00, -3.9513e-01,  7.3261e-01, -3.8526e-01,  5.4788e-01,\n",
            "         2.6681e+00, -7.9719e-02, -7.5132e-01,  1.3180e+00,  3.2902e+00,\n",
            "         2.4353e-02, -6.8258e-01, -3.2169e+00, -8.5364e-01,  1.9870e+00,\n",
            "         4.9420e-03,  4.1058e-01,  7.3612e-01,  1.5521e+00, -1.0788e-01,\n",
            "         9.1613e-01, -1.0040e+00, -5.3227e-02, -4.7267e-01,  2.6003e-01,\n",
            "        -3.0404e-01, -2.6544e-01,  6.6784e-01, -1.0443e+00, -1.0402e+00,\n",
            "         2.9968e-01,  3.7180e-01,  3.6411e-01, -3.6282e-01,  3.3519e-01,\n",
            "         1.3125e-01, -1.4848e+00,  2.9442e-01, -1.2704e+00, -5.4720e-01,\n",
            "        -8.6606e-01,  1.1620e+00,  4.1242e-01,  1.7189e+00, -1.8137e-01,\n",
            "         1.1565e+00,  2.9171e-01,  3.6375e-01, -1.2740e-01,  1.7415e-01,\n",
            "        -1.4941e+00,  8.1978e-01,  1.6304e-03,  6.0072e-01,  6.6593e-01,\n",
            "        -5.2381e-02,  5.7624e-01,  1.8456e+00,  1.3426e+00, -3.4707e-01,\n",
            "        -9.0099e-01,  1.2429e+00,  4.3168e-01, -7.3129e-01,  5.7563e-01,\n",
            "         2.0172e+00, -6.3395e-01, -2.1079e+00, -9.4497e-01,  1.3201e+00,\n",
            "         5.4175e-01,  1.1903e+00,  1.4632e+00, -1.2279e-01, -1.6250e+00,\n",
            "        -1.0760e+00,  1.5672e-01, -4.8080e-02,  2.9160e-01, -3.7308e-01,\n",
            "         6.1570e-01,  1.9310e-01, -8.4166e-01,  1.2352e-01,  1.5925e+00,\n",
            "         1.3811e+00, -5.7450e-01,  5.2375e-01,  1.3035e-01, -4.8324e-01,\n",
            "        -2.0130e+00,  4.7436e-01,  5.9171e-01,  1.3813e+00, -1.4067e-02,\n",
            "         2.7712e-01,  7.3090e-01,  1.3390e+00,  2.1433e+00, -1.3343e+00,\n",
            "         2.5672e-01,  1.4013e+00, -1.0345e+00, -1.3941e+00,  5.0075e-01,\n",
            "         7.1725e-01,  1.4886e+00, -7.9294e-01,  3.8556e-01, -4.3413e-01,\n",
            "        -6.0020e-01,  4.6741e-01, -1.8627e+00, -9.8626e-01, -8.2286e-01,\n",
            "         1.6822e-01, -3.8813e-02, -1.2398e+00,  2.4290e-01,  1.6346e+00,\n",
            "         2.1506e+00, -4.0787e-01,  6.7324e-01, -1.8148e+00,  5.2635e-01,\n",
            "         3.1027e-02,  1.4912e+00, -3.3216e-01, -1.8342e-01,  9.9350e-02,\n",
            "         2.1538e+00,  1.0668e+00, -1.3976e+00, -8.6166e-01, -7.6533e-01,\n",
            "         3.0014e-03], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 서비스\n",
            "tensor([ 0.0094,  1.0751, -1.5808,  2.0057,  0.8554, -0.2045, -1.5844, -2.0143,\n",
            "        -1.6028,  0.5114, -0.1680,  0.8425,  0.8364,  0.7468,  0.9938, -0.6743,\n",
            "        -1.2707,  0.6076, -0.6870,  0.7047,  1.2383, -1.5645,  0.1889,  0.0817,\n",
            "        -1.2269, -1.4768, -0.0058, -0.8238, -0.6919, -0.0478,  0.4900, -1.0578,\n",
            "         0.3560,  0.9091, -0.7990, -1.0702, -0.5722, -0.5500,  0.2822,  0.3002,\n",
            "         0.5979,  0.4041,  1.0075, -0.6775, -0.6157, -0.9706, -0.4935, -1.5039,\n",
            "        -0.3567,  1.3038,  0.3336, -0.5465, -0.5536,  0.9415,  1.9042,  1.9509,\n",
            "         0.1076,  0.6346, -0.0538, -1.4060,  1.2036, -0.5223, -0.0895,  1.6658,\n",
            "         3.1103, -0.0674,  0.2434,  2.1251, -1.3166,  0.2049, -0.9997, -0.4412,\n",
            "        -1.2131, -1.0717,  0.3595, -0.8089,  0.1263, -1.4760, -1.1926,  0.6037,\n",
            "         0.3873, -0.0638,  0.1418,  1.0155,  0.9335, -0.1948, -0.1737,  2.1708,\n",
            "        -0.0863, -0.5570,  2.0033, -1.2637,  1.1987,  0.5504, -0.6731,  0.1152,\n",
            "        -0.1786,  0.9386, -1.2627,  0.8746,  0.1592,  0.5876,  0.2336,  0.6163,\n",
            "        -0.0507,  2.6890, -2.0882,  0.1682,  0.0587,  0.4139,  0.6694,  1.3869,\n",
            "        -0.6274, -0.0426,  0.5849,  0.8808,  0.5120, -0.9522,  1.2120,  0.2112,\n",
            "         0.0146, -0.1527, -1.0322, -0.0460, -0.6677, -0.4690,  0.8165,  1.0485,\n",
            "        -0.6293,  1.5373,  2.0488, -0.1335,  0.1283, -0.1234,  0.0103,  1.6082,\n",
            "         0.4049,  0.5685, -0.1956,  0.9476, -0.1798, -0.0726, -0.2143,  1.2427,\n",
            "        -0.6979,  0.1929, -0.4003, -1.4375,  1.3086,  0.9963, -0.0582,  0.3496,\n",
            "        -1.6135, -0.6948, -0.8632,  0.9913, -0.6289, -0.8060, -0.4479, -1.3044,\n",
            "         0.1062, -0.5348,  0.7525, -1.8698, -1.3973, -0.6403, -0.1681, -2.0170,\n",
            "        -0.0519,  0.9522,  1.8608,  0.7592, -0.3438, -0.3669, -0.7335,  1.5320,\n",
            "         1.1565,  0.6748, -0.5437,  0.6973,  0.6736,  0.2037,  1.2431, -1.2389,\n",
            "         1.1721,  0.0291, -0.6854,  1.3691,  0.0299,  2.0848,  0.1809, -0.4931,\n",
            "        -0.2776, -0.7048,  1.1290, -0.5586, -1.1105, -0.2581, -0.5745,  0.3800,\n",
            "        -0.4822, -1.5733,  0.1328,  0.0757,  0.7634,  0.0073, -0.9852,  0.4803,\n",
            "        -0.8615,  1.0988,  0.2754, -1.0213,  0.7237, -0.6195, -1.4924,  0.7084,\n",
            "         0.7450,  1.0631, -1.6244, -0.7204,  1.0375,  0.5315, -1.2881,  0.4910,\n",
            "         1.2418, -0.0354, -0.5864,  1.5664,  0.8461, -0.4805,  0.4522,  0.4145,\n",
            "         0.4267, -0.0877, -0.0446, -0.4442,  0.0521, -0.1921, -0.4281,  1.2997,\n",
            "         0.3109,  1.8000, -1.0624,  0.8047,  1.3732,  0.4582,  1.9400, -0.8048,\n",
            "         0.7997, -0.2606,  0.8738, -0.2100,  0.6778,  1.0123,  0.2742,  0.5864],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 위생\n",
            "tensor([-1.8089,  0.8747,  1.3578, -2.0465,  0.6413,  0.4178, -0.8178, -1.3739,\n",
            "        -0.5372,  0.9414,  0.5867,  1.8408,  0.9530, -1.6606, -0.4063,  0.8017,\n",
            "        -0.1057,  0.7652,  0.4130, -0.6156,  1.6190, -0.3718,  0.8536, -0.3952,\n",
            "        -0.7639,  0.2756,  0.7815, -0.4130, -0.5685,  1.4489, -0.3813,  1.4141,\n",
            "        -1.3425, -0.5932,  0.6585, -2.6945,  0.9479,  0.6904,  1.5789, -1.2111,\n",
            "         1.2826,  0.5696, -0.1123, -0.9233,  0.1535, -0.8613,  1.2322, -0.3026,\n",
            "        -0.0361, -1.6314, -0.3910, -0.3602,  1.8790,  0.2414, -1.0627, -0.6716,\n",
            "        -1.0689,  0.0734,  0.4954, -0.1313,  2.2854,  1.0971,  0.2474,  0.6105,\n",
            "        -0.7142, -1.3831,  1.1535,  1.4346, -1.3956,  0.0386, -0.3715, -0.8073,\n",
            "        -0.7845, -0.9040,  0.6420, -0.6867,  1.0825,  1.3648, -1.4267, -0.9703,\n",
            "         0.5614,  1.0917, -1.0252,  1.0087, -1.6542, -0.9274, -0.7590, -0.1343,\n",
            "         0.1364, -0.2998, -1.5720, -0.0389,  2.1523, -0.7626,  0.0489,  1.1834,\n",
            "        -0.1501,  0.3272,  0.4272,  0.0134, -2.2114,  1.5442,  0.2117, -0.5252,\n",
            "        -1.0233,  1.7029,  0.1897,  0.5957, -1.5779, -1.3528,  0.4746,  0.5264,\n",
            "         1.5741, -1.3966,  0.6078, -0.9267, -0.1278, -0.5667, -0.9150, -0.0674,\n",
            "         0.7243,  0.8872,  0.0304,  0.1112, -1.0114,  0.1202,  0.4550,  1.1973,\n",
            "        -0.4669,  0.7688,  0.2760,  0.2640,  0.7092,  0.8033,  1.0114,  2.4606,\n",
            "        -0.2491, -0.2286,  1.7155,  0.7676, -0.6549, -0.6638,  0.3905,  0.6314,\n",
            "         0.8971, -0.1925, -1.2449, -1.1139, -0.0168,  0.0404,  0.0461, -0.1007,\n",
            "         0.8535,  0.0995,  0.0954,  0.4321, -0.0696,  1.1765, -0.1462,  0.3866,\n",
            "        -0.2941,  0.8065, -1.0965,  0.5172,  1.1096,  1.5076,  1.3372,  0.7705,\n",
            "        -0.5580,  0.9222,  0.6606,  0.3156,  1.2221,  0.8117, -1.3300,  0.8729,\n",
            "        -1.1544, -0.0787, -2.0536, -0.1373,  0.7564,  0.6105, -0.3302, -1.5523,\n",
            "        -1.5965,  0.0712,  0.2984, -0.9315,  0.7639,  0.5479,  0.4484, -0.1153,\n",
            "        -0.2827, -0.7282,  0.5035, -0.1021, -1.4411,  1.9106, -1.6964, -0.8442,\n",
            "        -0.0404,  1.2570, -0.9865,  1.6620,  0.2471, -0.9699,  0.8233,  0.7746,\n",
            "         0.8062, -0.7873,  1.1633, -2.2096, -0.2518, -1.2259,  1.1853, -1.4965,\n",
            "         0.7568,  0.3325,  1.1598, -1.3094,  0.3687, -0.0596,  0.2094,  0.3350,\n",
            "         0.8863, -0.1887, -0.1246,  0.5761, -0.5941,  0.4126, -0.6838, -0.3090,\n",
            "         0.0601, -1.0086, -0.1951,  1.7785,  0.6599, -0.0314, -0.5157,  0.5512,\n",
            "         1.4656,  3.1673,  1.0082,  0.1801, -0.6015,  0.3922, -0.4106, -0.5154,\n",
            "         0.4837,  0.6265, -0.2658, -0.1277, -0.9298,  1.5833,  0.5342,  0.8102],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
            "Word: 가격\n",
            "tensor([-1.1627e-01,  3.9256e-02,  1.6770e+00,  4.2642e-01, -5.7912e-01,\n",
            "         3.0498e-01,  1.3155e+00, -2.9160e+00,  9.5486e-01,  8.3404e-01,\n",
            "        -9.3834e-01, -8.0874e-01,  1.6250e+00,  5.9312e-01, -7.0985e-01,\n",
            "        -6.9135e-01, -2.1206e-01,  8.6122e-01, -1.9308e-01, -2.5170e-01,\n",
            "        -9.8479e-01, -6.6998e-01, -3.4440e-01, -2.2338e-01,  2.6565e-02,\n",
            "         1.8333e+00,  6.6553e-01,  4.2036e-01, -7.7465e-01,  6.6691e-01,\n",
            "        -1.4783e+00, -1.4146e+00, -1.1543e+00,  1.4302e+00, -6.3984e-01,\n",
            "        -1.1888e+00,  1.3410e+00, -5.1149e-01, -6.6873e-01,  1.8072e+00,\n",
            "        -3.6127e-01,  1.6531e+00, -7.0660e-01,  2.2365e+00, -4.1181e-01,\n",
            "        -1.7141e-01, -9.1123e-01, -7.5335e-01,  1.5126e-01, -1.0937e+00,\n",
            "         8.0986e-02, -2.2554e-01,  2.3097e-01, -5.3544e-01,  6.1703e-01,\n",
            "        -8.8960e-01,  5.6464e-01, -1.2525e+00, -1.2407e+00,  3.6197e-01,\n",
            "        -8.5330e-01,  1.2518e+00, -1.6297e+00, -4.7703e-01, -3.8515e-01,\n",
            "        -1.5777e+00,  1.6518e+00,  7.7740e-01, -8.7692e-02,  3.1451e-01,\n",
            "        -7.7642e-01, -9.5814e-01, -1.0089e+00, -9.6766e-01, -5.0104e-01,\n",
            "         6.2574e-01, -5.3202e-01,  2.1200e+00,  3.7474e-01,  3.3603e-02,\n",
            "         2.3236e-01, -3.1592e-01,  2.0303e+00,  3.3096e-01, -6.8864e-03,\n",
            "        -1.1086e+00,  1.0246e+00, -1.3684e-03,  1.6724e+00,  6.7301e-02,\n",
            "        -1.7218e-01,  5.9964e-01, -1.2114e-01, -6.0020e-01,  7.1087e-02,\n",
            "         2.1223e-01,  4.2519e-02, -1.4353e+00, -1.1167e+00,  2.8436e-01,\n",
            "         1.2104e+00, -1.8323e+00, -5.0289e-01,  6.7484e-02,  8.5974e-01,\n",
            "         1.1890e+00,  7.0573e-01, -5.3589e-01, -3.8312e-02, -1.4431e-01,\n",
            "         5.7629e-01,  1.3123e-01, -9.6539e-01, -2.5636e-01,  4.6675e-01,\n",
            "        -1.4495e+00,  1.4119e+00,  1.8434e+00,  8.7267e-01, -3.3718e-01,\n",
            "        -2.0343e-01, -9.8956e-01, -4.2945e-01, -6.7940e-02,  1.2203e+00,\n",
            "         1.6877e+00, -1.5691e+00,  7.7197e-01,  5.5540e-02,  2.0025e+00,\n",
            "         2.3656e-01, -2.0328e+00, -9.5097e-01,  5.4207e-01,  2.8942e-01,\n",
            "         7.5166e-02, -2.1469e-01,  6.0018e-01, -7.0942e-01, -5.7682e-01,\n",
            "        -9.0522e-01,  1.1268e+00,  6.1218e-01, -8.6262e-01, -6.9890e-01,\n",
            "         1.8100e-01, -2.1182e+00,  2.0258e-01, -2.7856e-02,  4.3211e-01,\n",
            "        -2.0785e-02,  4.9017e-01, -8.7834e-01,  1.4001e+00,  3.1462e-01,\n",
            "        -2.0498e-01,  9.0817e-02,  2.9821e-01, -1.8524e+00,  1.4417e+00,\n",
            "        -3.8047e-01,  9.6439e-01, -1.7783e+00,  2.2384e-01, -2.3189e+00,\n",
            "        -2.0946e+00,  7.3527e-01, -2.0829e-01, -1.0641e+00,  2.4707e+00,\n",
            "        -9.0272e-01,  5.1434e-01,  2.8702e-01, -1.3491e+00,  9.6980e-01,\n",
            "        -5.7530e-01, -3.5610e-01,  1.4997e-01, -4.4298e-01,  1.7095e+00,\n",
            "         7.8692e-02,  1.7340e-01,  1.7308e-01, -1.4037e+00, -5.1192e-01,\n",
            "         9.7493e-01,  6.1209e-01,  1.2748e-01,  1.3947e+00,  8.5029e-02,\n",
            "        -9.7129e-01, -1.1998e+00,  5.8278e-01, -1.0883e+00,  7.0206e-01,\n",
            "         4.6540e-02,  2.1579e-01, -7.8063e-01, -3.0946e-01,  3.0169e+00,\n",
            "         2.4183e-01,  5.0229e-01,  1.1362e+00,  5.6851e-01, -1.6092e-01,\n",
            "        -3.5020e-02, -1.8074e+00, -1.4632e+00,  1.1153e+00, -1.2728e+00,\n",
            "         6.4123e-01,  1.1960e+00, -2.3790e-01, -1.1394e+00,  1.4081e-01,\n",
            "         1.4423e+00,  5.4471e-02, -7.4090e-01, -1.1318e+00,  1.7662e+00,\n",
            "         3.0711e+00,  1.4124e+00,  6.6522e-01,  5.6056e-01,  1.5480e+00,\n",
            "         4.5971e-01,  5.7132e-01, -6.2974e-01, -6.9076e-01,  8.6316e-01,\n",
            "         9.5030e-01,  9.4890e-01, -1.4392e-01,  2.2508e-01,  1.1497e-01,\n",
            "         9.3856e-01, -4.6629e-01,  8.0144e-01,  4.6740e-01, -1.5765e+00,\n",
            "        -8.6905e-01, -2.0812e-01, -9.0506e-01, -4.3157e-01,  2.4689e-01,\n",
            "         5.0673e-01, -3.7220e-01, -2.1836e-01,  9.0283e-02,  7.5050e-01,\n",
            "        -5.6774e-02,  4.9764e-01,  1.5442e+00,  7.8879e-01, -1.9604e+00,\n",
            "         1.0132e+00], device='cuda:0', grad_fn=<SqueezeBackward1>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l5cPRZZe-R4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd44a1cc-c3e4-4cf0-8dfb-84a71d64d0e1"
      },
      "source": [
        "for word in test_words:\r\n",
        "  input_id = torch.LongTensor([w2i[word]]).to(device)\r\n",
        "  emb = skipgram.embedding(input_id)\r\n",
        "\r\n",
        "  print(f\"Word: {word}\")\r\n",
        "  print(max(emb.squeeze(0)))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word: 음식\n",
            "tensor(2.4956, device='cuda:0', grad_fn=<UnbindBackward>)\n",
            "Word: 맛\n",
            "tensor(2.5353, device='cuda:0', grad_fn=<UnbindBackward>)\n",
            "Word: 서비스\n",
            "tensor(2.5055, device='cuda:0', grad_fn=<UnbindBackward>)\n",
            "Word: 위생\n",
            "tensor(3.3142, device='cuda:0', grad_fn=<UnbindBackward>)\n",
            "Word: 가격\n",
            "tensor(3.4253, device='cuda:0', grad_fn=<UnbindBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmN4IchwisOO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e7b63e-9f94-4d18-cc21-ad608f8fa7bb"
      },
      "source": [
        "!apt-get install -qq texlive texlive-xetex texlive-latex-extra pandoc\r\n",
        "!pip install -qq pypandoc\r\n",
        "\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "!jupyter nbconvert --to PDF '/content/drive/My Drive/Colab Notebooks/1_naive_bayes.ipynb의 사본'"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 146442 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.8-2_all.deb ...\n",
            "Unpacking poppler-data (0.4.8-2) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.09_all.deb ...\n",
            "Unpacking tex-common (6.09) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../04-fonts-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../05-fonts-noto-mono_20171026-2_all.deb ...\n",
            "Unpacking fonts-noto-mono (20171026-2) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../06-fonts-texgyre_20160520-1_all.deb ...\n",
            "Unpacking fonts-texgyre (20160520-1) ...\n",
            "Selecting previously unselected package javascript-common.\n",
            "Preparing to unpack .../07-javascript-common_11_all.deb ...\n",
            "Unpacking javascript-common (11) ...\n",
            "Selecting previously unselected package libcupsfilters1:amd64.\n",
            "Preparing to unpack .../08-libcupsfilters1_1.20.2-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libcupsimage2:amd64.\n",
            "Preparing to unpack .../09-libcupsimage2_2.2.7-1ubuntu2.8_amd64.deb ...\n",
            "Unpacking libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../10-libijs-0.35_0.35-13_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-13) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../11-libjbig2dec0_0.13-6_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.13-6) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../12-libgs9-common_9.26~dfsg+0-0ubuntu0.18.04.14_all.deb ...\n",
            "Unpacking libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../13-libgs9_9.26~dfsg+0-0ubuntu0.18.04.14_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Selecting previously unselected package libjs-jquery.\n",
            "Preparing to unpack .../14-libjs-jquery_3.2.1-1_all.deb ...\n",
            "Unpacking libjs-jquery (3.2.1-1) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../15-libkpathsea6_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libpotrace0.\n",
            "Preparing to unpack .../16-libpotrace0_1.14-2_amd64.deb ...\n",
            "Unpacking libpotrace0 (1.14-2) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../17-libptexenc1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../18-rubygems-integration_1.11_all.deb ...\n",
            "Unpacking rubygems-integration (1.11) ...\n",
            "Selecting previously unselected package ruby2.5.\n",
            "Preparing to unpack .../19-ruby2.5_2.5.1-1ubuntu1.7_amd64.deb ...\n",
            "Unpacking ruby2.5 (2.5.1-1ubuntu1.7) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../20-ruby_1%3a2.5.1_amd64.deb ...\n",
            "Unpacking ruby (1:2.5.1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../21-rake_12.3.1-1ubuntu0.1_all.deb ...\n",
            "Unpacking rake (12.3.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-did-you-mean.\n",
            "Preparing to unpack .../22-ruby-did-you-mean_1.2.0-2_all.deb ...\n",
            "Unpacking ruby-did-you-mean (1.2.0-2) ...\n",
            "Selecting previously unselected package ruby-minitest.\n",
            "Preparing to unpack .../23-ruby-minitest_5.10.3-1_all.deb ...\n",
            "Unpacking ruby-minitest (5.10.3-1) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../24-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-power-assert.\n",
            "Preparing to unpack .../25-ruby-power-assert_0.3.0-1_all.deb ...\n",
            "Unpacking ruby-power-assert (0.3.0-1) ...\n",
            "Selecting previously unselected package ruby-test-unit.\n",
            "Preparing to unpack .../26-ruby-test-unit_3.2.5-1_all.deb ...\n",
            "Unpacking ruby-test-unit (3.2.5-1) ...\n",
            "Selecting previously unselected package libruby2.5:amd64.\n",
            "Preparing to unpack .../27-libruby2.5_2.5.1-1ubuntu1.7_amd64.deb ...\n",
            "Unpacking libruby2.5:amd64 (2.5.1-1ubuntu1.7) ...\n",
            "Selecting previously unselected package libsynctex1:amd64.\n",
            "Preparing to unpack .../28-libsynctex1_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexlua52:amd64.\n",
            "Preparing to unpack .../29-libtexlua52_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../30-libtexluajit2_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../31-libzzip-0-13_0.13.62-3.1ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../32-lmodern_2.004.5-3_all.deb ...\n",
            "Unpacking lmodern (2.004.5-3) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../33-preview-latex-style_11.91-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (11.91-1ubuntu1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../34-t1utils_1.41-2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-2) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../35-tex-gyre_20160520-1_all.deb ...\n",
            "Unpacking tex-gyre (20160520-1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../36-texlive-binaries_2017.20170613.44572-8ubuntu0.1_amd64.deb ...\n",
            "Unpacking texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../37-texlive-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../38-texlive-fonts-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../39-texlive-latex-base_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../40-texlive-latex-recommended_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive.\n",
            "Preparing to unpack .../41-texlive_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../42-texlive-pictures_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-pictures (2017.20180305-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../43-texlive-latex-extra_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-latex-extra (2017.20180305-2) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../44-texlive-plain-generic_2017.20180305-2_all.deb ...\n",
            "Unpacking texlive-plain-generic (2017.20180305-2) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../45-tipa_2%3a1.3-20_all.deb ...\n",
            "Unpacking tipa (2:1.3-20) ...\n",
            "Selecting previously unselected package texlive-xetex.\n",
            "Preparing to unpack .../46-texlive-xetex_2017.20180305-1_all.deb ...\n",
            "Unpacking texlive-xetex (2017.20180305-1) ...\n",
            "Setting up libgs9-common (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up libkpathsea6:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libjs-jquery (3.2.1-1) ...\n",
            "Setting up libtexlua52:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1) ...\n",
            "Setting up libsynctex1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up libptexenc1:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up tex-common (6.09) ...\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up poppler-data (0.4.8-2) ...\n",
            "Setting up tex-gyre (20160520-1) ...\n",
            "Setting up preview-latex-style (11.91-1ubuntu1) ...\n",
            "Setting up fonts-texgyre (20160520-1) ...\n",
            "Setting up fonts-noto-mono (20171026-2) ...\n",
            "Setting up fonts-lato (2.0-2) ...\n",
            "Setting up libcupsfilters1:amd64 (1.20.2-0ubuntu3.1) ...\n",
            "Setting up libcupsimage2:amd64 (2.2.7-1ubuntu2.8) ...\n",
            "Setting up libjbig2dec0:amd64 (0.13-6) ...\n",
            "Setting up ruby-did-you-mean (1.2.0-2) ...\n",
            "Setting up t1utils (1.41-2) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-13) ...\n",
            "Setting up rubygems-integration (1.11) ...\n",
            "Setting up libpotrace0 (1.14-2) ...\n",
            "Setting up javascript-common (11) ...\n",
            "Setting up ruby-minitest (5.10.3-1) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.62-3.1ubuntu0.18.04.1) ...\n",
            "Setting up libgs9:amd64 (9.26~dfsg+0-0ubuntu0.18.04.14) ...\n",
            "Setting up libtexluajit2:amd64 (2017.20170613.44572-8ubuntu0.1) ...\n",
            "Setting up fonts-lmodern (2.004.5-3) ...\n",
            "Setting up ruby-power-assert (0.3.0-1) ...\n",
            "Setting up texlive-binaries (2017.20170613.44572-8ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up texlive-base (2017.20180305-1) ...\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/config/pdftexconfig.tex\n",
            "Setting up texlive-fonts-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-plain-generic (2017.20180305-2) ...\n",
            "Setting up texlive-latex-base (2017.20180305-1) ...\n",
            "Setting up lmodern (2.004.5-3) ...\n",
            "Setting up texlive-latex-recommended (2017.20180305-1) ...\n",
            "Setting up texlive-pictures (2017.20180305-1) ...\n",
            "Setting up tipa (2:1.3-20) ...\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-DEBIAN'... done.\n",
            "Regenerating '/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST'... done.\n",
            "update-fmtutil has updated the following file(s):\n",
            "\t/var/lib/texmf/fmtutil.cnf-DEBIAN\n",
            "\t/var/lib/texmf/fmtutil.cnf-TEXLIVEDIST\n",
            "If you want to activate the changes in the above file(s),\n",
            "you should run fmtutil-sys or fmtutil.\n",
            "Setting up texlive (2017.20180305-1) ...\n",
            "Setting up texlive-latex-extra (2017.20180305-2) ...\n",
            "Setting up texlive-xetex (2017.20180305-1) ...\n",
            "Setting up ruby2.5 (2.5.1-1ubuntu1.7) ...\n",
            "Setting up ruby (1:2.5.1) ...\n",
            "Setting up ruby-test-unit (3.2.5-1) ...\n",
            "Setting up rake (12.3.1-1ubuntu0.1) ...\n",
            "Setting up libruby2.5:amd64 (2.5.1-1ubuntu1.7) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for fontconfig (2.12.6-0ubuntu2) ...\n",
            "Processing triggers for tex-common (6.09) ...\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "  Building wheel for pypandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Mounted at /content/drive\n",
            "[NbConvertApp] WARNING | pattern u'/content/drive/My Drive/Colab Notebooks/1_naive_bayes.ipynb\\uc758 \\uc0ac\\ubcf8' matched no files\n",
            "This application is used to convert notebook files (*.ipynb) to various other\n",
            "formats.\n",
            "\n",
            "WARNING: THE COMMANDLINE INTERFACE MAY CHANGE IN FUTURE RELEASES.\n",
            "\n",
            "Options\n",
            "-------\n",
            "\n",
            "Arguments that take values are actually convenience aliases to full\n",
            "Configurables, whose aliases are listed on the help line. For more information\n",
            "on full configurables, see '--help-all'.\n",
            "\n",
            "--execute\n",
            "    Execute the notebook prior to export.\n",
            "--allow-errors\n",
            "    Continue notebook execution even if one of the cells throws an error and include the error message in the cell output (the default behaviour is to abort conversion). This flag is only relevant if '--execute' was specified, too.\n",
            "--no-input\n",
            "    Exclude input cells and output prompts from converted document. \n",
            "    This mode is ideal for generating code-free reports.\n",
            "--stdout\n",
            "    Write notebook output to stdout instead of files.\n",
            "--stdin\n",
            "    read a single notebook file from stdin. Write the resulting notebook with default basename 'notebook.*'\n",
            "--inplace\n",
            "    Run nbconvert in place, overwriting the existing notebook (only \n",
            "    relevant when converting to notebook format)\n",
            "-y\n",
            "    Answer yes to any questions instead of prompting.\n",
            "--clear-output\n",
            "    Clear output of current file and save in place, \n",
            "    overwriting the existing notebook.\n",
            "--debug\n",
            "    set log level to logging.DEBUG (maximize logging output)\n",
            "--no-prompt\n",
            "    Exclude input and output prompts from converted document.\n",
            "--generate-config\n",
            "    generate default config file\n",
            "--nbformat=<Enum> (NotebookExporter.nbformat_version)\n",
            "    Default: 4\n",
            "    Choices: [1, 2, 3, 4]\n",
            "    The nbformat version to write. Use this to downgrade notebooks.\n",
            "--output-dir=<Unicode> (FilesWriter.build_directory)\n",
            "    Default: ''\n",
            "    Directory to write output(s) to. Defaults to output to the directory of each\n",
            "    notebook. To recover previous default behaviour (outputting to the current\n",
            "    working directory) use . as the flag value.\n",
            "--writer=<DottedObjectName> (NbConvertApp.writer_class)\n",
            "    Default: 'FilesWriter'\n",
            "    Writer class used to write the  results of the conversion\n",
            "--log-level=<Enum> (Application.log_level)\n",
            "    Default: 30\n",
            "    Choices: (0, 10, 20, 30, 40, 50, 'DEBUG', 'INFO', 'WARN', 'ERROR', 'CRITICAL')\n",
            "    Set the log level by value or name.\n",
            "--reveal-prefix=<Unicode> (SlidesExporter.reveal_url_prefix)\n",
            "    Default: u''\n",
            "    The URL prefix for reveal.js (version 3.x). This defaults to the reveal CDN,\n",
            "    but can be any url pointing to a copy  of reveal.js.\n",
            "    For speaker notes to work, this must be a relative path to a local  copy of\n",
            "    reveal.js: e.g., \"reveal.js\".\n",
            "    If a relative path is given, it must be a subdirectory of the current\n",
            "    directory (from which the server is run).\n",
            "    See the usage documentation\n",
            "    (https://nbconvert.readthedocs.io/en/latest/usage.html#reveal-js-html-\n",
            "    slideshow) for more details.\n",
            "--to=<Unicode> (NbConvertApp.export_format)\n",
            "    Default: 'html'\n",
            "    The export format to be used, either one of the built-in formats\n",
            "    ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf',\n",
            "    'python', 'rst', 'script', 'slides'] or a dotted object name that represents\n",
            "    the import path for an `Exporter` class\n",
            "--template=<Unicode> (TemplateExporter.template_file)\n",
            "    Default: u''\n",
            "    Name of the template file to use\n",
            "--output=<Unicode> (NbConvertApp.output_base)\n",
            "    Default: ''\n",
            "    overwrite base name use for output files. can only be used when converting\n",
            "    one notebook at a time.\n",
            "--post=<DottedOrNone> (NbConvertApp.postprocessor_class)\n",
            "    Default: u''\n",
            "    PostProcessor class used to write the results of the conversion\n",
            "--config=<Unicode> (JupyterApp.config_file)\n",
            "    Default: u''\n",
            "    Full path of a config file.\n",
            "\n",
            "To see all available configurables, use `--help-all`\n",
            "\n",
            "Examples\n",
            "--------\n",
            "\n",
            "    The simplest way to use nbconvert is\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb\n",
            "    \n",
            "    which will convert mynotebook.ipynb to the default format (probably HTML).\n",
            "    \n",
            "    You can specify the export format with `--to`.\n",
            "    Options include ['asciidoc', 'custom', 'html', 'latex', 'markdown', 'notebook', 'pdf', 'python', 'rst', 'script', 'slides'].\n",
            "    \n",
            "    > jupyter nbconvert --to latex mynotebook.ipynb\n",
            "    \n",
            "    Both HTML and LaTeX support multiple output templates. LaTeX includes\n",
            "    'base', 'article' and 'report'.  HTML includes 'basic' and 'full'. You\n",
            "    can specify the flavor of the format used.\n",
            "    \n",
            "    > jupyter nbconvert --to html --template basic mynotebook.ipynb\n",
            "    \n",
            "    You can also pipe the output to stdout, rather than a file\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb --stdout\n",
            "    \n",
            "    PDF is generated via latex\n",
            "    \n",
            "    > jupyter nbconvert mynotebook.ipynb --to pdf\n",
            "    \n",
            "    You can get (and serve) a Reveal.js-powered slideshow\n",
            "    \n",
            "    > jupyter nbconvert myslides.ipynb --to slides --post serve\n",
            "    \n",
            "    Multiple notebooks can be given at the command line in a couple of \n",
            "    different ways:\n",
            "    \n",
            "    > jupyter nbconvert notebook*.ipynb\n",
            "    > jupyter nbconvert notebook1.ipynb notebook2.ipynb\n",
            "    \n",
            "    or you can specify the notebooks list in a config file, containing::\n",
            "    \n",
            "        c.NbConvertApp.notebooks = [\"my_notebook.ipynb\"]\n",
            "    \n",
            "    > jupyter nbconvert --config mycfg.py\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91kdGfbVV9xp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
