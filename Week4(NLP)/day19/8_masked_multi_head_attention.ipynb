{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "8_masked_multi_head_attention.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyungminkim-dev/boostcamp-ai-tech/blob/main/8_masked_multi_head_attention_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "##**8. Masked Multi-head Attention**\r\n",
        "1. Masked Multi-head Attention 구현.\r\n",
        "2. Encoder-Decoder Attention 구현."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "import torch\r\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfTSaGYteuze"
      },
      "source": [
        "데이터의 값과 형태를 좀 더 명확하게 보기 위해 sample을 줄이겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\r\n",
        "vocab_size = 100\r\n",
        "\r\n",
        "data = [\r\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13],\r\n",
        "  [60, 96, 51, 32, 90],\r\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],\r\n",
        "  [66, 88, 98, 47],\r\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23]\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "def padding(data):\r\n",
        "  max_len = len(max(data, key=len))\r\n",
        "  print(f\"Maximum sequence length: {max_len}\")\r\n",
        "\r\n",
        "  for i, seq in enumerate(tqdm(data)):\r\n",
        "    if len(seq) < max_len:\r\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\r\n",
        "\r\n",
        "  return data, max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504e2a02-ff14-49d6-9411-2c804d5ad429"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:00<00:00, 25025.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c82b9c35-9bd3-4c7b-c4ee-dea6ecddfa4a"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0],\n",
              " [77, 65, 51, 77, 19, 15, 35, 19, 23, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### **Hyperparameter 세팅 및 embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 8  # model의 hidden size\r\n",
        "num_heads = 2  # head의 개수\r\n",
        "inf = 1e12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\r\n",
        "\r\n",
        "# B: batch size, L: maximum sequence length\r\n",
        "batch = torch.LongTensor(data)  # (B, L)\r\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ae854d-5712-4dc9-f5cd-802a18b07bdb"
      },
      "source": [
        "print(batch_emb)\r\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4447,  0.4835, -2.0615,  0.5273, -0.8963, -0.7690, -0.2828,\n",
            "           0.8650],\n",
            "         [ 0.1394,  0.8519, -0.3930, -1.4890, -0.2720,  0.4163, -0.9520,\n",
            "          -0.3072],\n",
            "         [-0.3814,  0.5316,  1.0158, -1.6750, -0.2991,  0.1610,  0.5263,\n",
            "          -0.9468],\n",
            "         [-0.7757,  0.1679, -1.0084,  0.0602,  1.8452, -1.4202,  1.9326,\n",
            "           0.9985],\n",
            "         [ 0.0543,  2.0459,  0.7251,  0.5859, -2.3143, -0.1585, -1.8926,\n",
            "           0.1609],\n",
            "         [-0.5554, -0.9443, -0.2000,  0.5699,  0.5639,  1.3002,  1.4385,\n",
            "          -0.9753],\n",
            "         [-0.9316,  1.3416, -0.0189,  1.3569,  0.2254, -0.7460, -0.5626,\n",
            "           1.5735],\n",
            "         [ 0.1394,  0.8519, -0.3930, -1.4890, -0.2720,  0.4163, -0.9520,\n",
            "          -0.3072],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485]],\n",
            "\n",
            "        [[-0.6865,  1.6435, -0.7536, -0.2426, -0.5635, -0.2042,  1.8953,\n",
            "          -0.8255],\n",
            "         [-0.7815, -1.4023,  0.5372,  1.1013,  0.7558, -0.8506,  0.1996,\n",
            "          -0.4424],\n",
            "         [-0.9820,  1.2862,  0.4119,  0.1587, -0.7759,  0.9945, -0.3299,\n",
            "           1.9860],\n",
            "         [-1.6409, -0.1513, -0.0561, -0.6096,  0.7747, -1.3041, -1.6818,\n",
            "           0.3112],\n",
            "         [-1.0931,  0.4661,  0.5988, -0.3325,  0.4030, -0.0977,  1.0595,\n",
            "          -0.9652],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485]],\n",
            "\n",
            "        [[-0.4574, -1.3909, -0.5434,  0.1985,  0.4540,  1.7965,  0.3969,\n",
            "          -1.2009],\n",
            "         [ 0.1334, -0.5018,  0.1079, -0.1145,  0.6104, -0.1889, -2.7115,\n",
            "          -0.9435],\n",
            "         [ 0.9871, -1.0997,  0.0758, -1.4067,  0.4496, -0.5460, -0.2187,\n",
            "          -2.1980],\n",
            "         [-0.8468,  0.0731,  2.0271,  1.4174, -0.8520,  0.5458, -2.1885,\n",
            "          -0.0538],\n",
            "         [-0.3164,  0.0204,  1.0335,  1.3647, -0.2717, -0.5479, -0.8145,\n",
            "          -0.3446],\n",
            "         [-0.9010, -1.4353, -0.9653,  1.8931, -1.8265, -0.6682, -0.9814,\n",
            "          -0.6029],\n",
            "         [ 0.0116,  1.2200, -1.3834, -1.5074, -1.1104, -3.1299,  0.5704,\n",
            "           0.5904],\n",
            "         [-0.7455,  0.6308, -1.9243,  0.5302,  0.0990, -0.4844,  0.0957,\n",
            "           0.9144],\n",
            "         [-0.9901,  0.4005,  0.6126,  1.0695, -0.9899, -0.5339,  1.0725,\n",
            "           0.1509],\n",
            "         [-0.5020,  0.2389,  0.7778,  0.4296, -0.2282, -0.3462,  0.3754,\n",
            "          -1.7146]],\n",
            "\n",
            "        [[-0.8144,  0.8913, -0.4820,  0.9782,  0.0879,  0.2067, -0.0256,\n",
            "           2.3385],\n",
            "         [ 0.5622, -1.4503,  1.7446, -0.0873, -2.2885,  0.0599,  0.6623,\n",
            "          -0.5666],\n",
            "         [-0.5346,  1.4214, -0.8822,  1.5818,  0.8076, -0.2123,  0.7445,\n",
            "           1.0174],\n",
            "         [-0.3814,  0.5316,  1.0158, -1.6750, -0.2991,  0.1610,  0.5263,\n",
            "          -0.9468],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485]],\n",
            "\n",
            "        [[ 0.8098,  0.7014, -0.4058,  0.3118, -0.0182,  0.2075,  0.5437,\n",
            "           0.1164],\n",
            "         [-0.8468,  0.0731,  2.0271,  1.4174, -0.8520,  0.5458, -2.1885,\n",
            "          -0.0538],\n",
            "         [-0.9820,  1.2862,  0.4119,  0.1587, -0.7759,  0.9945, -0.3299,\n",
            "           1.9860],\n",
            "         [ 0.8098,  0.7014, -0.4058,  0.3118, -0.0182,  0.2075,  0.5437,\n",
            "           0.1164],\n",
            "         [-0.6697,  1.2909,  0.5929, -0.7132,  0.7737, -0.7381, -1.1916,\n",
            "          -0.7200],\n",
            "         [ 0.4416, -0.4361, -0.5667, -0.5597, -0.0730, -0.2510,  0.9529,\n",
            "           0.0069],\n",
            "         [-0.4574, -1.3909, -0.5434,  0.1985,  0.4540,  1.7965,  0.3969,\n",
            "          -1.2009],\n",
            "         [-0.6697,  1.2909,  0.5929, -0.7132,  0.7737, -0.7381, -1.1916,\n",
            "          -0.7200],\n",
            "         [ 2.2057, -0.9435, -1.7270, -1.4954,  0.4964, -0.7348, -1.4315,\n",
            "          -0.3352],\n",
            "         [ 0.9032,  0.6662,  0.1377,  1.6429,  0.7820, -0.3549,  1.1131,\n",
            "           0.9485]]], grad_fn=<EmbeddingBackward>)\n",
            "torch.Size([5, 10, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dO3gxeyhpyF2"
      },
      "source": [
        "### **Mask 구축**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NDEQF64p5pN"
      },
      "source": [
        "`True`는 attention이 적용될 부분, `False`는 masking될 자리입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aB0A4elupM2g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a194ece-04a0-4671-e04d-22980a845b0f"
      },
      "source": [
        "padding_mask = (batch != pad_id).unsqueeze(1)  # (B, 1, L)\r\n",
        "\r\n",
        "print(padding_mask)\r\n",
        "print(padding_mask.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ True,  True,  True,  True,  True,  True,  True,  True, False, False]],\n",
            "\n",
            "        [[ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
            "\n",
            "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
            "\n",
            "        [[ True,  True,  True,  True, False, False, False, False, False, False]],\n",
            "\n",
            "        [[ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])\n",
            "torch.Size([5, 1, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88cD54evrEo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015137af-ae79-42b7-a8fb-47185808ccb8"
      },
      "source": [
        "nopeak_mask = torch.ones([1, max_len, max_len], dtype=torch.bool)  # (1, L, L)\r\n",
        "nopeak_mask = torch.tril(nopeak_mask)  # (1, L, L)\r\n",
        "\r\n",
        "print(nopeak_mask)\r\n",
        "print(nopeak_mask.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]]])\n",
            "torch.Size([1, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMzB8_jarycy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66dcdb51-5d8c-40c0-bbb4-5a7955598059"
      },
      "source": [
        "mask = padding_mask & nopeak_mask  # (B, L, L)\r\n",
        "\r\n",
        "print(mask)\r\n",
        "print(mask.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False]],\n",
            "\n",
            "        [[ True, False, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True, False, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True, False, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True, False, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True, False, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True, False, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True, False, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True, False, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False],\n",
            "         [ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]])\n",
            "torch.Size([5, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "### **Linear transformation & 여러 head로 나누기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\r\n",
        "w_k = nn.Linear(d_model, d_model)\r\n",
        "w_v = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cfbbb74-3e9b-453c-efe1-79ece65d9cc5"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\r\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\r\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\r\n",
        "\r\n",
        "batch_size = q.shape[0]\r\n",
        "d_k = d_model // num_heads\r\n",
        "\r\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2, 10, 4])\n",
            "torch.Size([5, 2, 10, 4])\n",
            "torch.Size([5, 2, 10, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### **Masking이 적용된 self-attention 구현**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqaQmVQdvMZB"
      },
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adlRCt6mvMy5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d53b12-d15c-4509-c081-e7d71711727a"
      },
      "source": [
        "masks = mask.unsqueeze(1)  # (B, 1, L, L)\r\n",
        "masked_attn_scores = attn_scores.masked_fill_(masks == False, -1 * inf)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "print(masked_attn_scores)\r\n",
        "print(masked_attn_scores.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[ 7.7431e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.2856e-01,  3.5391e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.1656e-02,  4.1102e-02,  9.6596e-02, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.9644e-01,  8.5291e-02,  1.5766e-01, -2.6219e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.9505e-01,  2.6274e-01,  2.5438e-01, -4.1808e-02,  7.6989e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-3.4542e-01, -3.0874e-01, -2.6850e-01,  9.8166e-02, -6.4603e-01,\n",
            "            7.5059e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.0460e-01,  1.1001e-01,  1.2974e-01, -1.1451e-01,  4.0976e-01,\n",
            "           -3.7673e-01,  4.8761e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.2856e-01,  3.5391e-01,  4.6300e-01,  2.3791e-01,  8.0067e-01,\n",
            "           -4.9410e-01,  3.7441e-01,  3.5391e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.2654e-01, -2.6941e-01, -2.2263e-01, -2.3428e-01, -3.9303e-01,\n",
            "            2.7510e-01, -3.5562e-01, -2.6941e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.2654e-01, -2.6941e-01, -2.2263e-01, -2.3428e-01, -3.9303e-01,\n",
            "            2.7510e-01, -3.5562e-01, -2.6941e-01, -1.0000e+12, -1.0000e+12]],\n",
            "\n",
            "         [[-3.2102e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 6.7615e-02, -2.7659e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.3719e-01,  1.3098e-01, -3.8809e-02, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.7214e-01, -1.0197e-01,  1.1282e-01, -8.5351e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.4325e-01, -1.2908e-01, -1.7716e-01,  7.0344e-01, -1.1669e+00,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 4.1586e-01,  1.2599e-01, -8.4206e-02, -1.4642e-01,  4.8155e-01,\n",
            "           -2.3352e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-7.2315e-01, -2.0440e-01,  1.3571e-01, -2.6195e-01, -2.9353e-01,\n",
            "            2.0284e-01, -4.5549e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 6.7615e-02, -2.7659e-02, -1.5309e-01,  4.1280e-01, -5.0891e-01,\n",
            "            2.0051e-01, -1.8072e-01, -2.7659e-02, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.9020e-01, -1.6387e-01,  6.7511e-02, -3.1623e-01, -2.8238e-01,\n",
            "            2.6282e-01, -2.7732e-01, -1.6387e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.9020e-01, -1.6387e-01,  6.7511e-02, -3.1623e-01, -2.8238e-01,\n",
            "            2.6282e-01, -2.7732e-01, -1.6387e-01, -1.0000e+12, -1.0000e+12]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7822e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.8589e-01,  5.0018e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.6644e-01, -1.7521e-01, -6.0510e-02, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.1639e-01, -9.8944e-02,  7.3788e-01,  5.9402e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.7265e-02,  2.7248e-01, -1.1481e-01,  1.2708e-01,  1.2378e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.3491e-01,  1.4008e-01, -4.8926e-01, -2.2005e-01, -3.2114e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.3491e-01,  1.4008e-01, -4.8926e-01, -2.2005e-01, -3.2114e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.3491e-01,  1.4008e-01, -4.8926e-01, -2.2005e-01, -3.2114e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.3491e-01,  1.4008e-01, -4.8926e-01, -2.2005e-01, -3.2114e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.3491e-01,  1.4008e-01, -4.8926e-01, -2.2005e-01, -3.2114e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]],\n",
            "\n",
            "         [[-2.4968e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.0285e-02, -3.1906e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.1260e-01,  3.2483e-01, -7.4275e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-5.2207e-02,  1.3384e-01, -5.8497e-01, -7.3752e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.9366e-01, -1.6068e-01,  9.7276e-02,  4.3528e-02, -1.9528e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-5.9483e-01,  2.7767e-01,  8.7501e-02, -1.0485e-01, -8.2571e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-5.9483e-01,  2.7767e-01,  8.7501e-02, -1.0485e-01, -8.2571e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-5.9483e-01,  2.7767e-01,  8.7501e-02, -1.0485e-01, -8.2571e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-5.9483e-01,  2.7767e-01,  8.7501e-02, -1.0485e-01, -8.2571e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-5.9483e-01,  2.7767e-01,  8.7501e-02, -1.0485e-01, -8.2571e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]]],\n",
            "\n",
            "\n",
            "        [[[ 6.3425e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.4175e-01,  2.2595e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.2045e-01,  3.9081e-01, -9.7039e-02, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 5.4670e-01,  3.5774e-01, -1.7965e-02,  2.8869e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.2400e-01,  3.3863e-01, -1.4400e-01,  4.6834e-01,  2.6140e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.4934e-01,  4.3163e-01, -4.2030e-01,  9.8289e-01,  5.8593e-01,\n",
            "            7.3929e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.8729e+00, -4.3977e-01, -4.2705e-01,  3.2434e-01,  2.0713e-01,\n",
            "            2.6845e-01,  3.2594e+00, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-9.5263e-01, -2.3152e-01, -1.3866e-01,  1.6330e-01,  1.4770e-01,\n",
            "            2.0669e-01,  1.7440e+00,  2.2524e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.3253e-01,  1.9971e-01, -3.8001e-02,  2.0683e-01,  1.1775e-01,\n",
            "            1.6130e-01, -4.5332e-01,  9.7651e-03, -3.9174e-02, -1.0000e+12],\n",
            "          [ 2.7226e-01,  4.1754e-01, -1.2637e-01,  5.6695e-01,  3.4634e-01,\n",
            "            5.0689e-01, -5.6135e-01,  8.3118e-02,  8.5131e-02,  2.5202e-01]],\n",
            "\n",
            "         [[-1.8248e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.6220e-01,  3.4938e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.1075e-01, -1.1949e-01, -5.7190e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 6.4289e-02, -2.2397e-02,  3.1332e-01, -6.2605e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.2164e-01,  7.8252e-02,  4.8830e-02, -1.1324e-01, -6.9291e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.6181e-01,  1.5814e-01, -3.6861e-01,  1.6250e-01, -3.4485e-02,\n",
            "            5.8463e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.2740e-01,  4.9210e-01,  5.1429e-01,  2.2136e-01,  2.3551e-01,\n",
            "            3.3607e-01, -5.1811e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.1095e-01,  5.2844e-01,  5.2325e-01,  5.5615e-01,  3.9805e-01,\n",
            "            2.8984e-01, -6.7070e-01, -5.5559e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.4629e-02, -8.6851e-02, -1.5334e-01,  1.0628e-02, -3.6860e-02,\n",
            "           -1.4694e-02,  6.7081e-02,  6.0033e-02,  5.0410e-02, -1.0000e+12],\n",
            "          [ 1.5902e-01, -4.5426e-02, -2.7533e-01, -2.1416e-01, -1.9666e-01,\n",
            "            2.5514e-01,  4.3196e-02,  4.3147e-01, -1.1994e-01, -3.7014e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.1653e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.2325e-02, -5.9734e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.2058e-01,  2.8196e-01, -1.9637e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 8.6240e-02,  3.3256e-02,  1.8144e-01,  9.6596e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.0992e-01, -1.0872e-01, -1.7414e-01, -2.2263e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.0992e-01, -1.0872e-01, -1.7414e-01, -2.2263e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.0992e-01, -1.0872e-01, -1.7414e-01, -2.2263e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.0992e-01, -1.0872e-01, -1.7414e-01, -2.2263e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.0992e-01, -1.0872e-01, -1.7414e-01, -2.2263e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-4.0992e-01, -1.0872e-01, -1.7414e-01, -2.2263e-01, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]],\n",
            "\n",
            "         [[-7.0722e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 9.1238e-01, -9.8392e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-5.2420e-01,  8.6389e-01, -6.0212e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.8518e-01, -6.1516e-01,  4.2597e-01, -3.8809e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.9635e-02,  6.9745e-01, -4.3965e-01,  6.7511e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.9635e-02,  6.9745e-01, -4.3965e-01,  6.7511e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.9635e-02,  6.9745e-01, -4.3965e-01,  6.7511e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.9635e-02,  6.9745e-01, -4.3965e-01,  6.7511e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.9635e-02,  6.9745e-01, -4.3965e-01,  6.7511e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-6.9635e-02,  6.9745e-01, -4.3965e-01,  6.7511e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12]]],\n",
            "\n",
            "\n",
            "        [[[ 1.8678e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.5229e-01,  2.8869e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.0437e-01, -5.3697e-01, -6.0510e-02, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.8678e-02,  1.5068e-02, -4.1438e-02,  1.8678e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.4769e-02,  2.7853e-01,  3.7538e-01, -1.4769e-02,  2.2443e-01,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.0439e-01,  1.7035e-01,  1.5867e-01,  1.0439e-01,  1.9188e-01,\n",
            "            1.1771e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.3361e-02,  3.6538e-01, -2.7342e-01,  2.3361e-02,  2.1752e-01,\n",
            "           -1.0320e-01,  6.3425e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-1.4769e-02,  2.7853e-01,  3.7538e-01, -1.4769e-02,  2.2443e-01,\n",
            "            3.0070e-02, -4.9576e-01,  2.2443e-01, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.5799e-01,  4.8649e-01,  8.7096e-01,  1.5799e-01,  5.2441e-01,\n",
            "            2.9062e-01, -1.1709e+00,  5.2441e-01,  7.4136e-02, -1.0000e+12],\n",
            "          [-1.6771e-01, -5.5132e-02, -4.8926e-01, -1.6771e-01, -2.0945e-01,\n",
            "           -1.5433e-01,  3.4503e-01, -2.0945e-01, -1.5996e-01, -1.0000e+12]],\n",
            "\n",
            "         [[ 3.4227e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.1496e-01, -6.2605e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 5.5154e-02, -3.0113e-01, -7.4275e-01, -1.0000e+12, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 3.4227e-02,  1.1180e-02, -6.3127e-02,  3.4227e-02, -1.0000e+12,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.2622e-01, -2.3449e-01, -6.5536e-01,  2.2622e-01, -8.5149e-02,\n",
            "           -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-2.0800e-02,  3.8816e-02,  2.7097e-01, -2.0800e-02, -1.2614e-01,\n",
            "           -3.5197e-02, -1.0000e+12, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [-7.8982e-03, -6.9589e-02,  5.1325e-01, -7.8982e-03, -5.9896e-02,\n",
            "           -1.2466e-01, -1.8248e-01, -1.0000e+12, -1.0000e+12, -1.0000e+12],\n",
            "          [ 2.2622e-01, -2.3449e-01, -6.5536e-01,  2.2622e-01, -8.5149e-02,\n",
            "            2.4726e-01,  2.6204e-01, -8.5149e-02, -1.0000e+12, -1.0000e+12],\n",
            "          [ 1.7891e-01, -1.8382e-02, -6.2703e-02,  1.7891e-01, -6.0660e-01,\n",
            "            2.4967e-01,  5.5886e-01, -6.0660e-01,  9.0308e-01, -1.0000e+12],\n",
            "          [-1.3670e-01,  4.3245e-01,  8.7501e-02, -1.3670e-01, -3.0220e-01,\n",
            "            3.1103e-02,  3.5750e-01, -3.0220e-01, -7.4221e-02, -1.0000e+12]]]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n",
            "torch.Size([5, 2, 10, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EqMuVJFwHhI"
      },
      "source": [
        "`-1* inf`로 masking된 부분은 softmax 후 0이 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVNze4elv4Uf"
      },
      "source": [
        "attn_dists = F.softmax(masked_attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "print(attn_dists)\r\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBwm34bswV7e"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Xab7WKzTEU"
      },
      "source": [
        "### **전체 코드**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlF7R6DIzVWc"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(MultiheadAttention, self).__init__()\r\n",
        "\r\n",
        "    # Q, K, V learnable matrices\r\n",
        "    self.w_q = nn.Linear(d_model, d_model)\r\n",
        "    self.w_k = nn.Linear(d_model, d_model)\r\n",
        "    self.w_v = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "    # Linear transformation for concatenated outputs\r\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "  def forward(self, q, k, v, mask=None):\r\n",
        "    batch_size = q.shape[0]\r\n",
        "\r\n",
        "    q = self.w_q(q)  # (B, L, d_model)\r\n",
        "    k = self.w_k(k)  # (B, L, d_model)\r\n",
        "    v = self.w_v(v)  # (B, L, d_model)\r\n",
        "\r\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    attn_values = self.self_attention(q, k, v, mask=mask)  # (B, num_heads, L, d_k)\r\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\r\n",
        "\r\n",
        "    return self.w_0(attn_values)\r\n",
        "\r\n",
        "  def self_attention(self, q, k, v, mask=None):\r\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "    if mask is not None:\r\n",
        "      mask = mask.unsqueeze(1)  # (B, 1, L, L) or  (B, 1, 1, L)\r\n",
        "      attn_scores = attn_scores.masked_fill_(mask == False, -1*inf)\r\n",
        "\r\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    return attn_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\r\n",
        "\r\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb, mask=mask)  # (B, L, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB"
      },
      "source": [
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1g99JEEFwFv"
      },
      "source": [
        "### **Encoder-Decoder attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2PRoF4fF4ah"
      },
      "source": [
        "Query, key, value만 달라질 뿐 구현은 동일합니다.  \r\n",
        "Decoder에 들어갈 batch만 별도 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p26ra2BsGEdb"
      },
      "source": [
        "trg_data = [\r\n",
        "  [33, 11, 49, 10],\r\n",
        "  [88, 34, 5, 29, 99, 45, 11, 25],\r\n",
        "  [67, 25, 15, 90, 54, 4, 92, 10, 46, 20, 88 ,19],\r\n",
        "  [16, 58, 91, 47, 12, 5, 8],\r\n",
        "  [71, 63, 62, 7, 9, 11, 55, 91, 32, 48]\r\n",
        "]\r\n",
        "\r\n",
        "trg_data, trg_max_len = padding(trg_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYysB4EKHKGI"
      },
      "source": [
        "# S_L: source maximum sequence length, T_L: target maximum sequence length\r\n",
        "src_batch = batch  # (B, S_L)\r\n",
        "trg_batch = torch.LongTensor(trg_data)  # (B, T_L)\r\n",
        "\r\n",
        "print(src_batch.shape)\r\n",
        "print(trg_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AieDxWYIHXKc"
      },
      "source": [
        "src_emb = embedding(src_batch)  # (B, S_L, d_w)\r\n",
        "trg_emb = embedding(trg_batch)  # (B, T_L, d_w)\r\n",
        "\r\n",
        "print(src_emb.shape)\r\n",
        "print(trg_emb.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxCjmPurH2b7"
      },
      "source": [
        "`src_emb`를 encoder에서 나온 결과, 그리고 `trg_emb`를 masked multi-head attention 후 결과로 가정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUhY-z8JHeUE"
      },
      "source": [
        "q = w_q(trg_emb)  # (B, T_L, d_model)\r\n",
        "k = w_k(src_emb)  # (B, S_L, d_model)\r\n",
        "v = w_v(src_emb)  # (B, S_L, d_model)\r\n",
        "\r\n",
        "batch_size = q.shape[0]\r\n",
        "d_k = d_model // num_heads\r\n",
        "\r\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, T_L, num_heads, d_k)\r\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)\r\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, S_L, num_heads, d_k)\r\n",
        "\r\n",
        "q = q.transpose(1, 2)  # (B, num_heads, T_L, d_k)\r\n",
        "k = k.transpose(1, 2)  # (B, num_heads, S_L, d_k)\r\n",
        "v = v.transpose(1, 2)  # (B, num_heads, S_L, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeqjkVqkIdxO"
      },
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, T_L, S_L)\r\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, T_L, S_L)\r\n",
        "\r\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQv4IINbItS0"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, T_L, d_k)\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLCHeCbtJDy9"
      },
      "source": [
        "Masked multi-head attention 후 나온 결과와 동일한 shape를 가지며 이후 layer에서 전체 연산도 동일하게 진행됩니다."
      ]
    }
  ]
}
