{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "7_multi_head_attention.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyungminkim-dev/boostcamp-ai-tech/blob/main/7_multi_head_attention_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KsBGZpKkWki"
      },
      "source": [
        "##**7. Multi-head Attention**\r\n",
        "1. Multi-head attention 및 self-attention 구현.\r\n",
        "2. 각 과정에서 일어나는 연산과 input/output 형태 이해."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qRU5DFY2OM8"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDtMioSQQ1bB"
      },
      "source": [
        "from torch import nn\r\n",
        "from torch.nn import functional as F\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "import torch\r\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBiZObgRep_Q"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9ULZIqTenSc"
      },
      "source": [
        "pad_id = 0\r\n",
        "vocab_size = 100\r\n",
        "\r\n",
        "data = [\r\n",
        "  [62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75],\r\n",
        "  [60, 96, 51, 32, 90],\r\n",
        "  [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54],\r\n",
        "  [75, 51],\r\n",
        "  [66, 88, 98, 47],\r\n",
        "  [21, 39, 10, 64, 21],\r\n",
        "  [98],\r\n",
        "  [77, 65, 51, 77, 19, 15, 35, 19, 23, 97, 50, 46, 53, 42, 45, 91, 66, 3, 43, 10],\r\n",
        "  [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34],\r\n",
        "  [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43]\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hx3mcivgMyH"
      },
      "source": [
        "def padding(data):\r\n",
        "  max_len = len(max(data, key=len))\r\n",
        "  print(f\"Maximum sequence length: {max_len}\")\r\n",
        "\r\n",
        "  for i, seq in enumerate(tqdm(data)):\r\n",
        "    if len(seq) < max_len:\r\n",
        "      data[i] = seq + [pad_id] * (max_len - len(seq))\r\n",
        "\r\n",
        "  return data, max_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3e8FiNvgX60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c151c567-1e02-409f-acdd-e781c7f85981"
      },
      "source": [
        "data, max_len = padding(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 39681.21it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Maximum sequence length: 20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwPSIWYugaN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf22569-4935-48cf-a70d-9944007e4989"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[62, 13, 47, 39, 78, 33, 56, 13, 39, 29, 44, 86, 71, 36, 18, 75, 0, 0, 0, 0],\n",
              " [60, 96, 51, 32, 90, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [35, 45, 48, 65, 91, 99, 92, 10, 3, 21, 54, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [75, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [66, 88, 98, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [21, 39, 10, 64, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [98, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [77,\n",
              "  65,\n",
              "  51,\n",
              "  77,\n",
              "  19,\n",
              "  15,\n",
              "  35,\n",
              "  19,\n",
              "  23,\n",
              "  97,\n",
              "  50,\n",
              "  46,\n",
              "  53,\n",
              "  42,\n",
              "  45,\n",
              "  91,\n",
              "  66,\n",
              "  3,\n",
              "  43,\n",
              "  10],\n",
              " [70, 64, 98, 25, 99, 53, 4, 13, 69, 62, 66, 76, 15, 75, 45, 34, 0, 0, 0, 0],\n",
              " [20, 64, 81, 35, 76, 85, 1, 62, 8, 45, 99, 77, 19, 43, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwqjACx8iidc"
      },
      "source": [
        "### **Hyperparameter 세팅 및 embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Ngp2nWimS8"
      },
      "source": [
        "d_model = 512  # model의 hidden size\r\n",
        "num_heads = 8  # head의 개수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJMi2Xsni5uq"
      },
      "source": [
        "embedding = nn.Embedding(vocab_size, d_model)\r\n",
        "\r\n",
        "# B: batch size, L: maximum sequence length\r\n",
        "batch = torch.LongTensor(data)  # (B, L)\r\n",
        "batch_emb = embedding(batch)  # (B, L, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tLCUQwojcUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "525b92a0-802e-474e-a48d-5e835737ce7b"
      },
      "source": [
        "print(batch_emb)\r\n",
        "print(batch_emb.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-0.3130, -0.6466, -0.6364,  ...,  0.4092,  0.7185,  0.8237],\n",
            "         [ 1.6666,  0.6779, -0.8941,  ..., -0.2906,  1.4590, -0.5031],\n",
            "         [-0.7811,  0.8180,  0.6448,  ..., -0.0807,  1.2186,  0.2906],\n",
            "         ...,\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754],\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754],\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754]],\n",
            "\n",
            "        [[ 0.1669, -0.8360,  1.5726,  ..., -1.4015,  2.0910, -0.9003],\n",
            "         [-1.3691,  0.6413,  0.2234,  ...,  0.0307, -0.5075, -1.0385],\n",
            "         [ 1.2657,  0.3973, -0.4212,  ...,  0.9322,  1.0555, -0.2589],\n",
            "         ...,\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754],\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754],\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754]],\n",
            "\n",
            "        [[-0.5233,  1.3509,  0.1022,  ..., -0.0256,  1.5855,  0.4957],\n",
            "         [ 0.7227, -0.0131, -1.5200,  ...,  0.7671,  0.1152,  0.6024],\n",
            "         [-0.4667,  0.0438,  1.5187,  ..., -0.7449,  0.5825, -0.2153],\n",
            "         ...,\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754],\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754],\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 2.1491,  1.0098,  0.1035,  ...,  0.2388,  0.2493, -1.4190],\n",
            "         [ 0.9044, -0.3984, -0.0927,  ..., -1.5748,  2.1248, -0.2480],\n",
            "         [ 1.2657,  0.3973, -0.4212,  ...,  0.9322,  1.0555, -0.2589],\n",
            "         ...,\n",
            "         [-1.1484,  1.1636,  0.7666,  ..., -1.9144, -0.0356, -1.1097],\n",
            "         [ 1.5250, -1.5920,  1.2976,  ...,  1.7630,  0.3207, -1.2192],\n",
            "         [-1.2246, -0.2721, -0.3742,  ..., -0.1750,  0.1539, -1.0876]],\n",
            "\n",
            "        [[-0.6222,  0.3213,  1.7669,  ..., -0.6147, -0.0311, -0.7456],\n",
            "         [ 1.5301, -0.2146,  0.7889,  ...,  0.4190,  0.4284,  2.3016],\n",
            "         [-0.2920,  0.9851,  0.3383,  ..., -0.8438,  0.6371,  0.0985],\n",
            "         ...,\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754],\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754],\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754]],\n",
            "\n",
            "        [[ 1.3365, -1.5780, -1.6921,  ..., -1.4399,  0.2837,  1.0967],\n",
            "         [ 1.5301, -0.2146,  0.7889,  ...,  0.4190,  0.4284,  2.3016],\n",
            "         [ 2.0922, -0.2459,  1.8431,  ..., -0.6005, -0.8085, -0.2391],\n",
            "         ...,\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754],\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754],\n",
            "         [-1.2335, -1.0142, -0.8085,  ..., -0.9706, -1.2609,  1.3754]]],\n",
            "       grad_fn=<EmbeddingBackward>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Lhx892gmi3"
      },
      "source": [
        "### **Linear transformation & 여러 head로 나누기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urXMBRnRgqvw"
      },
      "source": [
        "Multi-head attention 내에서 쓰이는 linear transformation matrix들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DWKDqgCgfMk"
      },
      "source": [
        "w_q = nn.Linear(d_model, d_model)\r\n",
        "w_k = nn.Linear(d_model, d_model)\r\n",
        "w_v = nn.Linear(d_model, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcLuhda7m-Lm"
      },
      "source": [
        "w_0 = nn.Linear(d_model, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-vSL7PwnV6k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab613368-ff21-4798-937e-049caca25913"
      },
      "source": [
        "q = w_q(batch_emb)  # (B, L, d_model)\r\n",
        "k = w_k(batch_emb)  # (B, L, d_model)\r\n",
        "v = w_v(batch_emb)  # (B, L, d_model)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wnvlum-LnF1T"
      },
      "source": [
        "Q, k, v를 `num_head`개의 차원 분할된 여러 vector로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tiOKAv9nEli",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f060b9d4-bc10-4cd3-d9e7-00bb3e853341"
      },
      "source": [
        "batch_size = q.shape[0]\r\n",
        "d_k = d_model // num_heads\r\n",
        "\r\n",
        "q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n",
            "torch.Size([10, 20, 8, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tNb2isfn5Cx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33cbc09f-3165-4e4b-ee56-7f2977643e19"
      },
      "source": [
        "q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "print(q.shape)\r\n",
        "print(k.shape)\r\n",
        "print(v.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n",
            "torch.Size([10, 8, 20, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWrDA5_Sofad"
      },
      "source": [
        "### **Scaled dot-product self-attention 구현**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w52C4k3Wfl8m"
      },
      "source": [
        "각 head에서 실행되는 self-attetion 과정입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5waKr0Hfi2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a9fbcd-7ac6-4c0b-b97c-bb2ea1af7f38"
      },
      "source": [
        "attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\r\n",
        "attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "print(attn_dists)\r\n",
        "print(attn_dists.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[[0.0730, 0.0249, 0.0589,  ..., 0.0396, 0.0396, 0.0396],\n",
            "          [0.0851, 0.0448, 0.0288,  ..., 0.0371, 0.0371, 0.0371],\n",
            "          [0.0614, 0.0490, 0.0504,  ..., 0.0252, 0.0252, 0.0252],\n",
            "          ...,\n",
            "          [0.0506, 0.0413, 0.0359,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0506, 0.0413, 0.0359,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0506, 0.0413, 0.0359,  ..., 0.0433, 0.0433, 0.0433]],\n",
            "\n",
            "         [[0.0404, 0.0511, 0.0407,  ..., 0.0616, 0.0616, 0.0616],\n",
            "          [0.0303, 0.0663, 0.0474,  ..., 0.0398, 0.0398, 0.0398],\n",
            "          [0.0704, 0.0413, 0.0385,  ..., 0.0379, 0.0379, 0.0379],\n",
            "          ...,\n",
            "          [0.0553, 0.0407, 0.0458,  ..., 0.0453, 0.0453, 0.0453],\n",
            "          [0.0553, 0.0407, 0.0458,  ..., 0.0453, 0.0453, 0.0453],\n",
            "          [0.0553, 0.0407, 0.0458,  ..., 0.0453, 0.0453, 0.0453]],\n",
            "\n",
            "         [[0.0469, 0.0534, 0.0682,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0702, 0.0597, 0.0324,  ..., 0.0644, 0.0644, 0.0644],\n",
            "          [0.0436, 0.0659, 0.0606,  ..., 0.0531, 0.0531, 0.0531],\n",
            "          ...,\n",
            "          [0.0371, 0.0507, 0.0559,  ..., 0.0436, 0.0436, 0.0436],\n",
            "          [0.0371, 0.0507, 0.0559,  ..., 0.0436, 0.0436, 0.0436],\n",
            "          [0.0371, 0.0507, 0.0559,  ..., 0.0436, 0.0436, 0.0436]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0190, 0.0649, 0.0409,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0518, 0.0367, 0.0560,  ..., 0.0430, 0.0430, 0.0430],\n",
            "          [0.0447, 0.0164, 0.0503,  ..., 0.0831, 0.0831, 0.0831],\n",
            "          ...,\n",
            "          [0.0266, 0.0287, 0.0658,  ..., 0.0580, 0.0580, 0.0580],\n",
            "          [0.0266, 0.0287, 0.0658,  ..., 0.0580, 0.0580, 0.0580],\n",
            "          [0.0266, 0.0287, 0.0658,  ..., 0.0580, 0.0580, 0.0580]],\n",
            "\n",
            "         [[0.0375, 0.0381, 0.0670,  ..., 0.0610, 0.0610, 0.0610],\n",
            "          [0.0360, 0.0650, 0.0503,  ..., 0.0465, 0.0465, 0.0465],\n",
            "          [0.0386, 0.0350, 0.0409,  ..., 0.0860, 0.0860, 0.0860],\n",
            "          ...,\n",
            "          [0.0664, 0.0439, 0.0508,  ..., 0.0512, 0.0512, 0.0512],\n",
            "          [0.0664, 0.0439, 0.0508,  ..., 0.0512, 0.0512, 0.0512],\n",
            "          [0.0664, 0.0439, 0.0508,  ..., 0.0512, 0.0512, 0.0512]],\n",
            "\n",
            "         [[0.0294, 0.0256, 0.0482,  ..., 0.0371, 0.0371, 0.0371],\n",
            "          [0.0393, 0.0212, 0.0341,  ..., 0.0537, 0.0537, 0.0537],\n",
            "          [0.0417, 0.0671, 0.0398,  ..., 0.0296, 0.0296, 0.0296],\n",
            "          ...,\n",
            "          [0.0797, 0.0481, 0.0447,  ..., 0.0446, 0.0446, 0.0446],\n",
            "          [0.0797, 0.0481, 0.0447,  ..., 0.0446, 0.0446, 0.0446],\n",
            "          [0.0797, 0.0481, 0.0447,  ..., 0.0446, 0.0446, 0.0446]]],\n",
            "\n",
            "\n",
            "        [[[0.0594, 0.0359, 0.1070,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          [0.0450, 0.0303, 0.0475,  ..., 0.0536, 0.0536, 0.0536],\n",
            "          [0.1008, 0.0497, 0.0489,  ..., 0.0455, 0.0455, 0.0455],\n",
            "          ...,\n",
            "          [0.0541, 0.0965, 0.0424,  ..., 0.0495, 0.0495, 0.0495],\n",
            "          [0.0541, 0.0965, 0.0424,  ..., 0.0495, 0.0495, 0.0495],\n",
            "          [0.0541, 0.0965, 0.0424,  ..., 0.0495, 0.0495, 0.0495]],\n",
            "\n",
            "         [[0.0642, 0.0533, 0.0568,  ..., 0.0499, 0.0499, 0.0499],\n",
            "          [0.0784, 0.0777, 0.0576,  ..., 0.0450, 0.0450, 0.0450],\n",
            "          [0.0645, 0.0746, 0.0619,  ..., 0.0469, 0.0469, 0.0469],\n",
            "          ...,\n",
            "          [0.0554, 0.0481, 0.0775,  ..., 0.0482, 0.0482, 0.0482],\n",
            "          [0.0554, 0.0481, 0.0775,  ..., 0.0482, 0.0482, 0.0482],\n",
            "          [0.0554, 0.0481, 0.0775,  ..., 0.0482, 0.0482, 0.0482]],\n",
            "\n",
            "         [[0.1291, 0.0940, 0.0649,  ..., 0.0358, 0.0358, 0.0358],\n",
            "          [0.0347, 0.0543, 0.0483,  ..., 0.0505, 0.0505, 0.0505],\n",
            "          [0.0467, 0.0640, 0.0638,  ..., 0.0463, 0.0463, 0.0463],\n",
            "          ...,\n",
            "          [0.0636, 0.0675, 0.0655,  ..., 0.0475, 0.0475, 0.0475],\n",
            "          [0.0636, 0.0675, 0.0655,  ..., 0.0475, 0.0475, 0.0475],\n",
            "          [0.0636, 0.0675, 0.0655,  ..., 0.0475, 0.0475, 0.0475]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0439, 0.0325, 0.0341,  ..., 0.0488, 0.0488, 0.0488],\n",
            "          [0.0283, 0.0341, 0.0445,  ..., 0.0521, 0.0521, 0.0521],\n",
            "          [0.0304, 0.0301, 0.0382,  ..., 0.0565, 0.0565, 0.0565],\n",
            "          ...,\n",
            "          [0.0372, 0.0258, 0.0560,  ..., 0.0535, 0.0535, 0.0535],\n",
            "          [0.0372, 0.0258, 0.0560,  ..., 0.0535, 0.0535, 0.0535],\n",
            "          [0.0372, 0.0258, 0.0560,  ..., 0.0535, 0.0535, 0.0535]],\n",
            "\n",
            "         [[0.0589, 0.0874, 0.1016,  ..., 0.0408, 0.0408, 0.0408],\n",
            "          [0.0268, 0.0368, 0.0627,  ..., 0.0540, 0.0540, 0.0540],\n",
            "          [0.0618, 0.0491, 0.0613,  ..., 0.0480, 0.0480, 0.0480],\n",
            "          ...,\n",
            "          [0.0480, 0.0303, 0.0354,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          [0.0480, 0.0303, 0.0354,  ..., 0.0515, 0.0515, 0.0515],\n",
            "          [0.0480, 0.0303, 0.0354,  ..., 0.0515, 0.0515, 0.0515]],\n",
            "\n",
            "         [[0.0580, 0.0566, 0.0794,  ..., 0.0472, 0.0472, 0.0472],\n",
            "          [0.0544, 0.0337, 0.0813,  ..., 0.0480, 0.0480, 0.0480],\n",
            "          [0.0731, 0.0484, 0.0507,  ..., 0.0497, 0.0497, 0.0497],\n",
            "          ...,\n",
            "          [0.0345, 0.0629, 0.0460,  ..., 0.0468, 0.0468, 0.0468],\n",
            "          [0.0345, 0.0629, 0.0460,  ..., 0.0468, 0.0468, 0.0468],\n",
            "          [0.0345, 0.0629, 0.0460,  ..., 0.0468, 0.0468, 0.0468]]],\n",
            "\n",
            "\n",
            "        [[[0.0593, 0.0640, 0.0347,  ..., 0.0376, 0.0376, 0.0376],\n",
            "          [0.0464, 0.0772, 0.0444,  ..., 0.0517, 0.0517, 0.0517],\n",
            "          [0.0403, 0.0691, 0.0318,  ..., 0.0545, 0.0545, 0.0545],\n",
            "          ...,\n",
            "          [0.0554, 0.0743, 0.0475,  ..., 0.0442, 0.0442, 0.0442],\n",
            "          [0.0554, 0.0743, 0.0475,  ..., 0.0442, 0.0442, 0.0442],\n",
            "          [0.0554, 0.0743, 0.0475,  ..., 0.0442, 0.0442, 0.0442]],\n",
            "\n",
            "         [[0.0501, 0.0362, 0.0602,  ..., 0.0587, 0.0587, 0.0587],\n",
            "          [0.0336, 0.0427, 0.0499,  ..., 0.0555, 0.0555, 0.0555],\n",
            "          [0.0678, 0.0404, 0.0375,  ..., 0.0544, 0.0544, 0.0544],\n",
            "          ...,\n",
            "          [0.0648, 0.0802, 0.0366,  ..., 0.0450, 0.0450, 0.0450],\n",
            "          [0.0648, 0.0802, 0.0366,  ..., 0.0450, 0.0450, 0.0450],\n",
            "          [0.0648, 0.0802, 0.0366,  ..., 0.0450, 0.0450, 0.0450]],\n",
            "\n",
            "         [[0.0451, 0.0296, 0.0615,  ..., 0.0476, 0.0476, 0.0476],\n",
            "          [0.0542, 0.0579, 0.0474,  ..., 0.0419, 0.0419, 0.0419],\n",
            "          [0.0448, 0.0275, 0.0654,  ..., 0.0597, 0.0597, 0.0597],\n",
            "          ...,\n",
            "          [0.0574, 0.0609, 0.0776,  ..., 0.0405, 0.0405, 0.0405],\n",
            "          [0.0574, 0.0609, 0.0776,  ..., 0.0405, 0.0405, 0.0405],\n",
            "          [0.0574, 0.0609, 0.0776,  ..., 0.0405, 0.0405, 0.0405]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0904, 0.0381, 0.0397,  ..., 0.0565, 0.0565, 0.0565],\n",
            "          [0.0325, 0.0302, 0.0187,  ..., 0.0484, 0.0484, 0.0484],\n",
            "          [0.0433, 0.0804, 0.0948,  ..., 0.0341, 0.0341, 0.0341],\n",
            "          ...,\n",
            "          [0.0416, 0.0306, 0.0256,  ..., 0.0561, 0.0561, 0.0561],\n",
            "          [0.0416, 0.0306, 0.0256,  ..., 0.0561, 0.0561, 0.0561],\n",
            "          [0.0416, 0.0306, 0.0256,  ..., 0.0561, 0.0561, 0.0561]],\n",
            "\n",
            "         [[0.0595, 0.0556, 0.0511,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0361, 0.0260, 0.0489,  ..., 0.0643, 0.0643, 0.0643],\n",
            "          [0.0279, 0.0267, 0.0489,  ..., 0.0596, 0.0596, 0.0596],\n",
            "          ...,\n",
            "          [0.0642, 0.0444, 0.0413,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0642, 0.0444, 0.0413,  ..., 0.0501, 0.0501, 0.0501],\n",
            "          [0.0642, 0.0444, 0.0413,  ..., 0.0501, 0.0501, 0.0501]],\n",
            "\n",
            "         [[0.0281, 0.0352, 0.0444,  ..., 0.0481, 0.0481, 0.0481],\n",
            "          [0.0675, 0.0481, 0.1076,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          [0.0622, 0.0299, 0.0256,  ..., 0.0648, 0.0648, 0.0648],\n",
            "          ...,\n",
            "          [0.0303, 0.0579, 0.0558,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          [0.0303, 0.0579, 0.0558,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          [0.0303, 0.0579, 0.0558,  ..., 0.0470, 0.0470, 0.0470]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.0438, 0.0724, 0.0553,  ..., 0.0600, 0.0307, 0.0351],\n",
            "          [0.0461, 0.0501, 0.0404,  ..., 0.0170, 0.0780, 0.0583],\n",
            "          [0.0368, 0.0590, 0.0326,  ..., 0.0382, 0.0399, 0.0732],\n",
            "          ...,\n",
            "          [0.0329, 0.0273, 0.0520,  ..., 0.0991, 0.0389, 0.0470],\n",
            "          [0.0579, 0.1101, 0.0534,  ..., 0.0372, 0.0418, 0.0734],\n",
            "          [0.0463, 0.0359, 0.0529,  ..., 0.0591, 0.0519, 0.0519]],\n",
            "\n",
            "         [[0.0325, 0.1227, 0.0479,  ..., 0.0907, 0.0375, 0.0577],\n",
            "          [0.0471, 0.0543, 0.0816,  ..., 0.0741, 0.0520, 0.0354],\n",
            "          [0.0529, 0.0520, 0.0688,  ..., 0.0450, 0.0464, 0.0564],\n",
            "          ...,\n",
            "          [0.0436, 0.0376, 0.0539,  ..., 0.0384, 0.0401, 0.0441],\n",
            "          [0.0426, 0.0301, 0.0451,  ..., 0.0435, 0.0468, 0.0407],\n",
            "          [0.0325, 0.0244, 0.0643,  ..., 0.0432, 0.0427, 0.0699]],\n",
            "\n",
            "         [[0.0738, 0.0530, 0.0656,  ..., 0.0290, 0.0322, 0.0362],\n",
            "          [0.0525, 0.0530, 0.0497,  ..., 0.0546, 0.0228, 0.0478],\n",
            "          [0.0354, 0.0381, 0.0575,  ..., 0.0612, 0.0393, 0.0643],\n",
            "          ...,\n",
            "          [0.0464, 0.0510, 0.0669,  ..., 0.0305, 0.0389, 0.0529],\n",
            "          [0.0571, 0.0607, 0.0450,  ..., 0.0444, 0.0893, 0.0341],\n",
            "          [0.1095, 0.0614, 0.0413,  ..., 0.0314, 0.0743, 0.0325]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0402, 0.0793, 0.0263,  ..., 0.0230, 0.0563, 0.0703],\n",
            "          [0.0845, 0.0385, 0.0269,  ..., 0.0399, 0.0335, 0.0469],\n",
            "          [0.0532, 0.0279, 0.0459,  ..., 0.0401, 0.0713, 0.0410],\n",
            "          ...,\n",
            "          [0.0760, 0.0577, 0.0400,  ..., 0.0654, 0.0537, 0.0598],\n",
            "          [0.0481, 0.0369, 0.0467,  ..., 0.0450, 0.0593, 0.0399],\n",
            "          [0.0448, 0.0300, 0.0516,  ..., 0.0586, 0.0394, 0.0264]],\n",
            "\n",
            "         [[0.0385, 0.0597, 0.1074,  ..., 0.0662, 0.0280, 0.0519],\n",
            "          [0.0400, 0.0751, 0.0438,  ..., 0.0536, 0.0607, 0.0539],\n",
            "          [0.0349, 0.0927, 0.0616,  ..., 0.0530, 0.0319, 0.0435],\n",
            "          ...,\n",
            "          [0.0708, 0.0261, 0.0328,  ..., 0.0441, 0.0332, 0.0668],\n",
            "          [0.0386, 0.0395, 0.0531,  ..., 0.0370, 0.0498, 0.0587],\n",
            "          [0.0577, 0.0358, 0.0530,  ..., 0.0265, 0.0488, 0.0605]],\n",
            "\n",
            "         [[0.0408, 0.0453, 0.1140,  ..., 0.0593, 0.0772, 0.0370],\n",
            "          [0.0648, 0.0461, 0.0526,  ..., 0.0461, 0.1058, 0.0644],\n",
            "          [0.0292, 0.0563, 0.0606,  ..., 0.0509, 0.0561, 0.0549],\n",
            "          ...,\n",
            "          [0.0254, 0.0321, 0.0727,  ..., 0.0907, 0.0418, 0.0432],\n",
            "          [0.0657, 0.0506, 0.0895,  ..., 0.0602, 0.0592, 0.0602],\n",
            "          [0.0646, 0.0501, 0.0880,  ..., 0.0479, 0.0458, 0.0447]]],\n",
            "\n",
            "\n",
            "        [[[0.0300, 0.0543, 0.0387,  ..., 0.0443, 0.0443, 0.0443],\n",
            "          [0.0505, 0.0640, 0.0291,  ..., 0.0619, 0.0619, 0.0619],\n",
            "          [0.0215, 0.0281, 0.0299,  ..., 0.0478, 0.0478, 0.0478],\n",
            "          ...,\n",
            "          [0.0249, 0.0719, 0.0326,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0249, 0.0719, 0.0326,  ..., 0.0427, 0.0427, 0.0427],\n",
            "          [0.0249, 0.0719, 0.0326,  ..., 0.0427, 0.0427, 0.0427]],\n",
            "\n",
            "         [[0.0390, 0.0541, 0.0582,  ..., 0.0399, 0.0399, 0.0399],\n",
            "          [0.0456, 0.0306, 0.0445,  ..., 0.0669, 0.0669, 0.0669],\n",
            "          [0.0347, 0.0393, 0.0483,  ..., 0.0837, 0.0837, 0.0837],\n",
            "          ...,\n",
            "          [0.0540, 0.0670, 0.0750,  ..., 0.0431, 0.0431, 0.0431],\n",
            "          [0.0540, 0.0670, 0.0750,  ..., 0.0431, 0.0431, 0.0431],\n",
            "          [0.0540, 0.0670, 0.0750,  ..., 0.0431, 0.0431, 0.0431]],\n",
            "\n",
            "         [[0.0461, 0.0358, 0.0708,  ..., 0.0659, 0.0659, 0.0659],\n",
            "          [0.0309, 0.0388, 0.0397,  ..., 0.0608, 0.0608, 0.0608],\n",
            "          [0.0475, 0.0333, 0.0385,  ..., 0.0792, 0.0792, 0.0792],\n",
            "          ...,\n",
            "          [0.0307, 0.0479, 0.0548,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0307, 0.0479, 0.0548,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0307, 0.0479, 0.0548,  ..., 0.0433, 0.0433, 0.0433]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.0522, 0.0854, 0.0777,  ..., 0.0228, 0.0228, 0.0228],\n",
            "          [0.0442, 0.0589, 0.0412,  ..., 0.0478, 0.0478, 0.0478],\n",
            "          [0.0618, 0.0628, 0.0408,  ..., 0.0279, 0.0279, 0.0279],\n",
            "          ...,\n",
            "          [0.0483, 0.0236, 0.0551,  ..., 0.0600, 0.0600, 0.0600],\n",
            "          [0.0483, 0.0236, 0.0551,  ..., 0.0600, 0.0600, 0.0600],\n",
            "          [0.0483, 0.0236, 0.0551,  ..., 0.0600, 0.0600, 0.0600]],\n",
            "\n",
            "         [[0.0925, 0.0610, 0.0635,  ..., 0.0388, 0.0388, 0.0388],\n",
            "          [0.0688, 0.0654, 0.0444,  ..., 0.0328, 0.0328, 0.0328],\n",
            "          [0.0714, 0.0592, 0.0948,  ..., 0.0468, 0.0468, 0.0468],\n",
            "          ...,\n",
            "          [0.0338, 0.0659, 0.0625,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          [0.0338, 0.0659, 0.0625,  ..., 0.0470, 0.0470, 0.0470],\n",
            "          [0.0338, 0.0659, 0.0625,  ..., 0.0470, 0.0470, 0.0470]],\n",
            "\n",
            "         [[0.0310, 0.0409, 0.0824,  ..., 0.0291, 0.0291, 0.0291],\n",
            "          [0.0435, 0.0390, 0.0398,  ..., 0.0559, 0.0559, 0.0559],\n",
            "          [0.0488, 0.0496, 0.0574,  ..., 0.0310, 0.0310, 0.0310],\n",
            "          ...,\n",
            "          [0.0760, 0.0465, 0.0547,  ..., 0.0468, 0.0468, 0.0468],\n",
            "          [0.0760, 0.0465, 0.0547,  ..., 0.0468, 0.0468, 0.0468],\n",
            "          [0.0760, 0.0465, 0.0547,  ..., 0.0468, 0.0468, 0.0468]]],\n",
            "\n",
            "\n",
            "        [[[0.0633, 0.0706, 0.0639,  ..., 0.0434, 0.0434, 0.0434],\n",
            "          [0.0583, 0.0646, 0.0398,  ..., 0.0625, 0.0625, 0.0625],\n",
            "          [0.0495, 0.0428, 0.0681,  ..., 0.0400, 0.0400, 0.0400],\n",
            "          ...,\n",
            "          [0.0373, 0.0677, 0.0516,  ..., 0.0402, 0.0402, 0.0402],\n",
            "          [0.0373, 0.0677, 0.0516,  ..., 0.0402, 0.0402, 0.0402],\n",
            "          [0.0373, 0.0677, 0.0516,  ..., 0.0402, 0.0402, 0.0402]],\n",
            "\n",
            "         [[0.0838, 0.0401, 0.0347,  ..., 0.0480, 0.0480, 0.0480],\n",
            "          [0.0530, 0.0302, 0.0417,  ..., 0.0660, 0.0660, 0.0660],\n",
            "          [0.0514, 0.0634, 0.0448,  ..., 0.0435, 0.0435, 0.0435],\n",
            "          ...,\n",
            "          [0.0603, 0.0673, 0.0397,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0603, 0.0673, 0.0397,  ..., 0.0433, 0.0433, 0.0433],\n",
            "          [0.0603, 0.0673, 0.0397,  ..., 0.0433, 0.0433, 0.0433]],\n",
            "\n",
            "         [[0.0821, 0.0328, 0.0676,  ..., 0.0483, 0.0483, 0.0483],\n",
            "          [0.0495, 0.0385, 0.0332,  ..., 0.0604, 0.0604, 0.0604],\n",
            "          [0.0265, 0.0465, 0.0584,  ..., 0.0367, 0.0367, 0.0367],\n",
            "          ...,\n",
            "          [0.0788, 0.0499, 0.0379,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          [0.0788, 0.0499, 0.0379,  ..., 0.0451, 0.0451, 0.0451],\n",
            "          [0.0788, 0.0499, 0.0379,  ..., 0.0451, 0.0451, 0.0451]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[0.1077, 0.0318, 0.0761,  ..., 0.0489, 0.0489, 0.0489],\n",
            "          [0.0557, 0.0583, 0.0472,  ..., 0.0473, 0.0473, 0.0473],\n",
            "          [0.0287, 0.0216, 0.0598,  ..., 0.0858, 0.0858, 0.0858],\n",
            "          ...,\n",
            "          [0.0461, 0.0223, 0.0375,  ..., 0.0568, 0.0568, 0.0568],\n",
            "          [0.0461, 0.0223, 0.0375,  ..., 0.0568, 0.0568, 0.0568],\n",
            "          [0.0461, 0.0223, 0.0375,  ..., 0.0568, 0.0568, 0.0568]],\n",
            "\n",
            "         [[0.0702, 0.0420, 0.0394,  ..., 0.0441, 0.0441, 0.0441],\n",
            "          [0.0537, 0.0517, 0.0690,  ..., 0.0260, 0.0260, 0.0260],\n",
            "          [0.0451, 0.0433, 0.0540,  ..., 0.0612, 0.0612, 0.0612],\n",
            "          ...,\n",
            "          [0.0542, 0.0643, 0.0431,  ..., 0.0458, 0.0458, 0.0458],\n",
            "          [0.0542, 0.0643, 0.0431,  ..., 0.0458, 0.0458, 0.0458],\n",
            "          [0.0542, 0.0643, 0.0431,  ..., 0.0458, 0.0458, 0.0458]],\n",
            "\n",
            "         [[0.0482, 0.0607, 0.0534,  ..., 0.0639, 0.0639, 0.0639],\n",
            "          [0.0345, 0.0413, 0.0530,  ..., 0.0591, 0.0591, 0.0591],\n",
            "          [0.0293, 0.0369, 0.0352,  ..., 0.0657, 0.0657, 0.0657],\n",
            "          ...,\n",
            "          [0.0469, 0.0415, 0.0357,  ..., 0.0417, 0.0417, 0.0417],\n",
            "          [0.0469, 0.0415, 0.0357,  ..., 0.0417, 0.0417, 0.0417],\n",
            "          [0.0469, 0.0415, 0.0357,  ..., 0.0417, 0.0417, 0.0417]]]],\n",
            "       grad_fn=<SoftmaxBackward>)\n",
            "torch.Size([10, 8, 20, 20])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7megouWpgCck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "544dc46e-741a-45f2-e6ce-fdeddeb2f4e9"
      },
      "source": [
        "attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 8, 20, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSTaymdg-P_"
      },
      "source": [
        "### **각 head의 결과물 병합**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSdQZCk0hCNd"
      },
      "source": [
        "각 head의 결과물을 concat하고 동일 차원으로 linear transformation합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaK0bpMGhQZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e11f1dc0-2ed5-4b68-b22c-602a532408b7"
      },
      "source": [
        "attn_values = attn_values.transpose(1, 2)  # (B, L, num_heads, d_k)\r\n",
        "attn_values = attn_values.contiguous().view(batch_size, -1, d_model)  # (B, L, d_model)\r\n",
        "\r\n",
        "print(attn_values.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTng_2SXhdH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c8cbeb-6395-41d4-fa3b-775d7916e624"
      },
      "source": [
        "outputs = w_0(attn_values)\r\n",
        "\r\n",
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[-3.2953e-01, -6.5045e-03,  5.5121e-02,  ...,  6.3708e-03,\n",
            "           1.0475e-02,  8.1765e-02],\n",
            "         [ 9.4638e-02, -8.5143e-03, -5.8232e-02,  ..., -1.6051e-01,\n",
            "          -1.5074e-01,  7.4297e-02],\n",
            "         [ 8.4103e-02, -4.2499e-02, -1.4555e-01,  ...,  9.6339e-03,\n",
            "           8.3171e-04,  6.2981e-02],\n",
            "         ...,\n",
            "         [-1.3716e-01,  6.6446e-02,  1.8032e-03,  ...,  1.9378e-01,\n",
            "          -1.7945e-01, -9.1599e-03],\n",
            "         [-1.9815e-01, -1.0303e-01, -2.4722e-02,  ...,  2.4668e-01,\n",
            "           1.2503e-01, -8.2504e-02],\n",
            "         [ 1.3707e-01, -8.2942e-02,  8.0011e-02,  ...,  1.5869e-01,\n",
            "          -1.2215e-02,  8.4874e-02]],\n",
            "\n",
            "        [[-5.8641e-01,  9.1846e-02,  1.4971e-01,  ..., -6.4201e-02,\n",
            "           1.8333e-01, -9.2120e-02],\n",
            "         [ 2.7623e-01, -2.9758e-02, -1.5169e-01,  ..., -3.2475e-01,\n",
            "          -2.6500e-01,  2.1315e-01],\n",
            "         [ 1.8831e-01,  4.3489e-03, -2.8373e-01,  ...,  3.2250e-01,\n",
            "          -1.1052e-02,  1.2384e-01],\n",
            "         ...,\n",
            "         [ 1.1189e-01,  3.3898e-01,  1.8307e-01,  ...,  3.2145e-01,\n",
            "          -2.7071e-01, -3.5693e-02],\n",
            "         [-4.5108e-01, -3.2511e-01,  4.3077e-01,  ...,  1.2461e-02,\n",
            "           1.6341e-01,  9.9851e-03],\n",
            "         [ 3.8413e-01, -9.8551e-02,  3.4489e-01,  ...,  3.7086e-01,\n",
            "          -2.0814e-01,  5.5076e-02]],\n",
            "\n",
            "        [[-3.6246e-01, -4.2477e-02,  9.1808e-02,  ..., -4.5260e-02,\n",
            "           2.2396e-01, -1.4626e-02],\n",
            "         [ 1.1652e-01,  4.2292e-02, -1.5681e-01,  ..., -1.3861e-01,\n",
            "          -1.4562e-01,  1.2366e-01],\n",
            "         [ 2.0803e-01, -1.1118e-01, -8.2308e-02,  ...,  8.2799e-02,\n",
            "          -1.3344e-01, -7.0409e-02],\n",
            "         ...,\n",
            "         [ 7.4285e-02,  1.6645e-01,  1.5815e-01,  ...,  1.5765e-01,\n",
            "          -2.3578e-01, -1.4290e-01],\n",
            "         [-1.2842e-01, -1.9874e-01,  3.8631e-01,  ...,  1.2489e-01,\n",
            "           1.4127e-01, -3.8968e-05],\n",
            "         [ 2.0419e-01, -2.6663e-02,  8.3794e-02,  ...,  1.3332e-01,\n",
            "          -1.9189e-01, -1.0106e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1714e-01, -4.9635e-02, -3.7905e-02,  ..., -5.8509e-02,\n",
            "           4.8363e-02, -5.0809e-02],\n",
            "         [ 1.0249e-01, -6.3952e-02, -1.0152e-01,  ..., -9.3875e-04,\n",
            "          -7.2557e-02,  5.4126e-02],\n",
            "         [-1.4521e-01, -2.1994e-01,  2.8264e-02,  ...,  8.6700e-02,\n",
            "          -5.0305e-02, -4.4497e-02],\n",
            "         ...,\n",
            "         [ 7.8908e-02, -1.5379e-01,  1.6799e-01,  ..., -4.5828e-02,\n",
            "          -1.9214e-01, -1.5579e-01],\n",
            "         [ 1.1313e-01,  1.0872e-02, -1.3674e-02,  ..., -2.6736e-02,\n",
            "           1.0690e-01,  1.6346e-01],\n",
            "         [-1.3254e-01, -1.8859e-02, -1.7673e-01,  ...,  7.5681e-02,\n",
            "          -1.6452e-02, -9.3811e-02]],\n",
            "\n",
            "        [[-1.7688e-01, -4.3131e-02, -7.4603e-04,  ..., -7.4834e-02,\n",
            "           7.8255e-03, -4.1948e-02],\n",
            "         [ 7.3353e-02, -7.4801e-02, -9.2277e-02,  ..., -7.8622e-02,\n",
            "          -2.0215e-01,  1.2721e-02],\n",
            "         [ 1.4687e-01, -8.4898e-02, -9.7941e-02,  ..., -1.3543e-02,\n",
            "          -8.5786e-02, -9.4949e-03],\n",
            "         ...,\n",
            "         [-8.4884e-02,  1.1891e-01,  4.2220e-02,  ...,  1.1853e-01,\n",
            "          -2.1936e-01, -2.9139e-02],\n",
            "         [-7.5213e-02, -1.3447e-01,  5.1980e-02,  ..., -6.5477e-02,\n",
            "          -4.5305e-03,  9.9579e-02],\n",
            "         [ 1.5261e-01, -2.5899e-02, -8.8084e-02,  ..., -3.8128e-02,\n",
            "          -1.6523e-01,  1.4820e-02]],\n",
            "\n",
            "        [[-2.0013e-01, -1.3047e-02,  1.9838e-01,  ..., -7.7042e-03,\n",
            "           1.7310e-01, -2.3925e-02],\n",
            "         [ 2.0844e-01, -2.5235e-02,  6.9084e-04,  ..., -2.3730e-01,\n",
            "          -1.5991e-01,  1.3597e-01],\n",
            "         [ 7.8757e-02, -9.1918e-02, -1.2180e-01,  ..., -5.4370e-03,\n",
            "          -8.0566e-02,  3.5201e-02],\n",
            "         ...,\n",
            "         [ 3.7658e-02,  8.4887e-02,  1.7861e-01,  ...,  2.5808e-01,\n",
            "          -8.2833e-02, -7.8938e-02],\n",
            "         [-2.3564e-01, -1.3178e-01,  1.9487e-01,  ..., -5.2836e-02,\n",
            "           3.2044e-02,  8.0364e-02],\n",
            "         [ 1.4954e-01, -1.9320e-02, -4.2899e-02,  ...,  9.4838e-02,\n",
            "          -2.0346e-01,  1.6471e-03]]], grad_fn=<AddBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goX70VKqhxQH"
      },
      "source": [
        "### **전체 코드**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtNyV7mMj7V_"
      },
      "source": [
        "위의 과정을 모두 합쳐 하나의 Multi-head attention 모듈을 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_kNhOTrkBHm"
      },
      "source": [
        "class MultiheadAttention(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(MultiheadAttention, self).__init__()\r\n",
        "\r\n",
        "    # Q, K, V learnable matrices\r\n",
        "    self.w_q = nn.Linear(d_model, d_model)\r\n",
        "    self.w_k = nn.Linear(d_model, d_model)\r\n",
        "    self.w_v = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "    # Linear transformation for concatenated outputs\r\n",
        "    self.w_0 = nn.Linear(d_model, d_model)\r\n",
        "\r\n",
        "  def forward(self, q, k, v):\r\n",
        "    batch_size = q.shape[0]\r\n",
        "\r\n",
        "    q = self.w_q(q)  # (B, L, d_model)\r\n",
        "    k = self.w_k(k)  # (B, L, d_model)\r\n",
        "    v = self.w_v(v)  # (B, L, d_model)\r\n",
        "\r\n",
        "    q = q.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    k = k.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "    v = v.view(batch_size, -1, num_heads, d_k)  # (B, L, num_heads, d_k)\r\n",
        "\r\n",
        "    q = q.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    k = k.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "    v = v.transpose(1, 2)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    attn_values = self.self_attention(q, k, v)  # (B, num_heads, L, d_k)\r\n",
        "    attn_values = attn_values.transpose(1, 2).contiguous().view(batch_size, -1, d_model)  # (B, L, num_heads, d_k) => (B, L, d_model)\r\n",
        "\r\n",
        "    return self.w_0(attn_values)\r\n",
        "\r\n",
        "  def self_attention(self, q, k, v):\r\n",
        "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)  # (B, num_heads, L, L)\r\n",
        "    attn_dists = F.softmax(attn_scores, dim=-1)  # (B, num_heads, L, L)\r\n",
        "\r\n",
        "    attn_values = torch.matmul(attn_dists, v)  # (B, num_heads, L, d_k)\r\n",
        "\r\n",
        "    return attn_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYLuu_9alQxT"
      },
      "source": [
        "multihead_attn = MultiheadAttention()\r\n",
        "\r\n",
        "outputs = multihead_attn(batch_emb, batch_emb, batch_emb)  # (B, L, d_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMiXlYjSlTfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62cdeb19-6632-40db-e7e8-83a5f7420a9e"
      },
      "source": [
        "print(outputs)\r\n",
        "print(outputs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0529, -0.0699,  0.0349,  ...,  0.0748, -0.0798,  0.0258],\n",
            "         [ 0.1503, -0.0778, -0.0345,  ...,  0.0374, -0.0475,  0.0037],\n",
            "         [ 0.0719, -0.0942,  0.0203,  ...,  0.0674, -0.0556, -0.0073],\n",
            "         ...,\n",
            "         [ 0.1416, -0.0343, -0.0094,  ...,  0.0688, -0.0589,  0.0386],\n",
            "         [ 0.1416, -0.0343, -0.0094,  ...,  0.0688, -0.0589,  0.0386],\n",
            "         [ 0.1416, -0.0343, -0.0094,  ...,  0.0688, -0.0589,  0.0386]],\n",
            "\n",
            "        [[ 0.1498, -0.0180,  0.1246,  ...,  0.2544,  0.2209,  0.0353],\n",
            "         [ 0.1812,  0.0320,  0.1634,  ...,  0.2869,  0.1846,  0.0479],\n",
            "         [ 0.1669,  0.0575,  0.1120,  ...,  0.2746,  0.2328,  0.0891],\n",
            "         ...,\n",
            "         [ 0.1625,  0.0239,  0.1223,  ...,  0.3176,  0.1976,  0.0513],\n",
            "         [ 0.1625,  0.0239,  0.1223,  ...,  0.3176,  0.1976,  0.0513],\n",
            "         [ 0.1625,  0.0239,  0.1223,  ...,  0.3176,  0.1976,  0.0513]],\n",
            "\n",
            "        [[ 0.1589,  0.0334,  0.2046,  ...,  0.2300,  0.1656, -0.0525],\n",
            "         [ 0.1099,  0.0454,  0.0962,  ...,  0.2138,  0.2490, -0.0431],\n",
            "         [ 0.1077,  0.0431,  0.2230,  ...,  0.2130,  0.1479, -0.0713],\n",
            "         ...,\n",
            "         [ 0.0704,  0.0853,  0.0930,  ...,  0.2929,  0.1460, -0.0048],\n",
            "         [ 0.0704,  0.0853,  0.0930,  ...,  0.2929,  0.1460, -0.0048],\n",
            "         [ 0.0704,  0.0853,  0.0930,  ...,  0.2929,  0.1460, -0.0048]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0718,  0.2648,  0.1342,  ...,  0.0516,  0.2347, -0.1158],\n",
            "         [ 0.0356,  0.2404,  0.1418,  ...,  0.0198,  0.1469, -0.1108],\n",
            "         [ 0.0568,  0.2677,  0.0943,  ...,  0.0364,  0.2121, -0.1291],\n",
            "         ...,\n",
            "         [ 0.0397,  0.2668,  0.0498,  ...,  0.0378,  0.1998, -0.1486],\n",
            "         [ 0.0474,  0.2217,  0.0924,  ...,  0.0556,  0.2139, -0.0930],\n",
            "         [ 0.1050,  0.2215,  0.0749,  ...,  0.0533,  0.1556, -0.1633]],\n",
            "\n",
            "        [[ 0.0836, -0.0599,  0.1772,  ...,  0.0804,  0.0098,  0.0534],\n",
            "         [ 0.1287, -0.0946,  0.1460,  ...,  0.0703,  0.0517,  0.0555],\n",
            "         [ 0.0817, -0.0275,  0.0695,  ...,  0.0547, -0.0041,  0.1518],\n",
            "         ...,\n",
            "         [ 0.1138, -0.0505,  0.1595,  ...,  0.1204,  0.0329,  0.1100],\n",
            "         [ 0.1138, -0.0505,  0.1595,  ...,  0.1204,  0.0329,  0.1100],\n",
            "         [ 0.1138, -0.0505,  0.1595,  ...,  0.1204,  0.0329,  0.1100]],\n",
            "\n",
            "        [[ 0.1146,  0.0441,  0.0175,  ...,  0.1297,  0.1427, -0.0886],\n",
            "         [ 0.0351, -0.0321,  0.0099,  ...,  0.1772,  0.2009, -0.1052],\n",
            "         [ 0.1053,  0.0157,  0.0831,  ...,  0.2036,  0.1490, -0.1370],\n",
            "         ...,\n",
            "         [ 0.0977,  0.0543,  0.0079,  ...,  0.1817,  0.1401, -0.0703],\n",
            "         [ 0.0977,  0.0543,  0.0079,  ...,  0.1817,  0.1401, -0.0703],\n",
            "         [ 0.0977,  0.0543,  0.0079,  ...,  0.1817,  0.1401, -0.0703]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "torch.Size([10, 20, 512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQcSL0uReUQ-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
