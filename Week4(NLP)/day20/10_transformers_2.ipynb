{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10_transformers_2.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyungminkim-dev/boostcamp-ai-tech/blob/main/10_transformers_2_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Iqy5hr9lYlh"
      },
      "source": [
        "##**10. Transformers 라이브러리 사용하기 2**\r\n",
        "1. GPT-2 모델 및 tokenizer 실습.\r\n",
        "2. Special token 추가."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci4yOyKBL_Cd"
      },
      "source": [
        "### **필요 패키지 import**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpHWOJoZlSt_"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEWH4HvNp7Qm"
      },
      "source": [
        "from transformers import *\r\n",
        "from torch import nn\r\n",
        "from tqdm import tqdm\r\n",
        "\r\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzuznNCMp_g0"
      },
      "source": [
        "### **GPT-2 불러오기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7lK48AMp8f7"
      },
      "source": [
        "gpt_name = 'gpt2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2731xAPqXZE"
      },
      "source": [
        "config = GPT2Config.from_pretrained(gpt_name)\r\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(gpt_name)\r\n",
        "model = GPT2Model.from_pretrained(gpt_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clTWxrdlqenD"
      },
      "source": [
        "config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ8AL8Y7q2Ok"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRIcb-Hkq5-z"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AnjbqKeq_Ye"
      },
      "source": [
        "### **Tokenizer 사용**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8CXqKgsq6j7"
      },
      "source": [
        "sentence = \"I want to go home.\"\r\n",
        "output = tokenizer(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSqq0jSerBgD"
      },
      "source": [
        "output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuYaImB8rCfc"
      },
      "source": [
        "tokenized = tokenizer.tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUX7HcXhrHpL"
      },
      "source": [
        "tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDOKzjBRrIZr"
      },
      "source": [
        "vocab = tokenizer.get_vocab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "641hc3TZsFcT"
      },
      "source": [
        "print(vocab)\r\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9AegCQErQ6k"
      },
      "source": [
        "vocab['<|endoftext|>']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaXPW89zrR3E"
      },
      "source": [
        "token_ids = [vocab[token] for token in tokenized]\r\n",
        "print(token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY26G6v1sM4k"
      },
      "source": [
        "token_ids = [tokenizer._convert_token_to_id(token) for token in tokenized]\r\n",
        "print(token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwXDAiV8sNxj"
      },
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokenized)\r\n",
        "print(token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiAtF1eCsO2E"
      },
      "source": [
        "token_ids = tokenizer.encode(sentence)\r\n",
        "print(token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na7mh6ZIsPu2"
      },
      "source": [
        "sentence = tokenizer.convert_tokens_to_string(tokenized)\r\n",
        "print(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1s5uFW2SsRrt"
      },
      "source": [
        "tokens = tokenizer.convert_ids_to_tokens(token_ids)\r\n",
        "print(tokens)\r\n",
        "sentence = tokenizer.convert_tokens_to_string(tokens)\r\n",
        "print(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwyTD8ZvTtAr"
      },
      "source": [
        "### **데이터 전처리**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4DHsQ2fspXN"
      },
      "source": [
        "마찬가지로 sample 데이터를 전처리합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siU8NdHns6j7"
      },
      "source": [
        "data = [\r\n",
        "  \"I want to go home.\",\r\n",
        "  \"My dog's name is Max.\",\r\n",
        "  \"Natural Language Processing is my favorite research field.\",\r\n",
        "  \"Welcome. How can I help you?\",\r\n",
        "  \"Shoot for the moon. Even if you miss, you'll land among the stars.\"\r\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Kl3RnB_s6z1"
      },
      "source": [
        "max_len = 0\r\n",
        "batch = []\r\n",
        "\r\n",
        "for sent in tqdm(data):\r\n",
        "  token_ids = tokenizer.encode(sent)\r\n",
        "  max_len = max(max_len, len(token_ids))\r\n",
        "  batch.append(token_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1q_nvCbs8Zo"
      },
      "source": [
        "pad_id = 0\r\n",
        "\r\n",
        "for i, token_ids in enumerate(tqdm(batch)):\r\n",
        "  if len(token_ids) < max_len:\r\n",
        "    batch[i] = token_ids + [pad_id] * (max_len-len(token_ids))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLqu2nDDx203"
      },
      "source": [
        "batch = torch.LongTensor(batch)\r\n",
        "\r\n",
        "print(batch)\r\n",
        "print(batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2rNH2htx3zs"
      },
      "source": [
        "batch_mask = (batch != pad_id).float()\r\n",
        "\r\n",
        "print(batch_mask)\r\n",
        "print(batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6FcXgIRdPz9"
      },
      "source": [
        "### **GPT-2 사용 및 응용**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1JiMhdMx90N"
      },
      "source": [
        "outputs = model(input_ids=batch, attention_mask=batch_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0DtTiwAykrQ"
      },
      "source": [
        "outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W51xZRbuylbW"
      },
      "source": [
        "# B: batch size, L: max length, d_h: hidden size\r\n",
        "last_hidden_states = outputs[0]  # (B, L, d_h)\r\n",
        "\r\n",
        "print(last_hidden_states.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q06d0WS7zARa"
      },
      "source": [
        "다음과 같이 fully connected layer를 하나 사용하여 다음 단어를 예측할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRmTXNMsyztw"
      },
      "source": [
        "lm_linear = nn.Linear(config.hidden_size, config.vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnenqteGzoFW"
      },
      "source": [
        "# V: vocab size\r\n",
        "lm_output = lm_linear(last_hidden_states)  # (B, L, V)\r\n",
        "\r\n",
        "print(lm_output)\r\n",
        "print(lm_output.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NYfcEqMz3cV"
      },
      "source": [
        "GPT-2 역시 다양한 head와 제공됩니다. (https://huggingface.co/transformers/model_doc/gpt2.html)  \r\n",
        "위의 Language Modeling을 동일하게 수행할 수 있는 모델은 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npnxu9Uhz0CM"
      },
      "source": [
        "lm_model = GPT2LMHeadModel.from_pretrained(gpt_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwWe5S3N0DtI"
      },
      "source": [
        "lm_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ9cfGlw0oxZ"
      },
      "source": [
        "GPT2LMHeadModel은 `input_ids`와 `labels`를 함께 줄 경우 자동으로 cross entropy loss까지 계산해줍니다.  \r\n",
        "`labels`가 주어지지 않을 경우엔 기존과 동일하게 결과만 주어집니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZWKIdQ40F4-"
      },
      "source": [
        "outputs = lm_model(input_ids=batch, attention_mask=batch_mask, labels=batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpxRre1W08lh"
      },
      "source": [
        "outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQswtcL809bW"
      },
      "source": [
        "loss = outputs[0]\r\n",
        "\r\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32sbDRju1Acn"
      },
      "source": [
        "logits = outputs[1]\r\n",
        "\r\n",
        "print(logits)\r\n",
        "print(logits.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8cKyL4E1ICb"
      },
      "source": [
        "### **Special token 추가하기**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaiPnWRc1NlB"
      },
      "source": [
        "경우에 따라선 별도의 special token을 추가하고 싶을 수 있습니다.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2qPe26Y1EqR"
      },
      "source": [
        "print(vocab)\r\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCe9-rrM1ndK"
      },
      "source": [
        "special_tokens = {\r\n",
        "    'bos_token': '[BOS]',\r\n",
        "    'eos_token': '[EOS]',\r\n",
        "    'pad_token': '[PAD]',\r\n",
        "    'additional_special_tokens': ['[SP1]', '[SP2]']\r\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj4BkgCP2E1h"
      },
      "source": [
        "num_new_tokens = tokenizer.add_special_tokens(special_tokens)\r\n",
        "print(num_new_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa3jdBTy2Ji3"
      },
      "source": [
        "vocab = tokenizer.get_vocab()\r\n",
        "print(vocab)\r\n",
        "print(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jyoMGkE4BUJ"
      },
      "source": [
        "tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMP33Sqb2kG2"
      },
      "source": [
        "Special token을 추가했다면 거기에 맞게 모델의 embedding layer의 input size도 바꿔주어야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX7Yaisa2QJK"
      },
      "source": [
        "model.resize_token_embeddings(len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzcrf-y52t8C"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
