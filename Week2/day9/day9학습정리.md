## day9 학습정리

* 딥러닝 확률론 

    * 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 둔다.

    * 예측이 틀릴 위험을 최소화하도록 데이터를 학습하는 원리는 통계적 기계학습의 기본 원리이다.

    * 회귀분석에서 사용하는 L-2노름은 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도한다. 

    * 분류문제에서 사용되는 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도한다.

* 이산확률변수 vs 연속확률변수

    * 확률변수는 확률분포에 따라 이산형와 연속형으로 구분된다.

    * 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링한다.

    * 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도위에서의 적분을 통해 모델링한다. (밀도는 누적활률변수의 변화율을 모델링하며 확률로 해석하면 안된다.)

* 조건부확률과 기계학습

    * 조건부확률 P(y|x)는 입력변수 x에 대해 정답이 y일 확률을 의미

    * 로지스틱 회귀에서 사용했던 선형모델괴 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용

    * 분류문제에서 softmax(W*pi(x)+b)은 데이터 x로부터 추출된 특징패턴 pi(x)과 가중치행렬 W을 통해 조건부확률 P(y|x)를 계산

    * 회귀문제의 경우 조건부기대값 E(y|x)을 추정


* 몬테카를로 샘플링 

    * 기계학습에서 많은 문제들은 확률분포를 명시적으로 모를 떄가 대부분이다. 

    * 확률분포를 모를 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로 샘플링 방법을 사용해야한다.

    * 몬테카를로 샘플링은 독립추출만 보장된다면 대수의 법칙에 의해 수렴성을 보장한다. 

    
